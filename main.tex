% !TeX document-id = {20a758e9-3325-489d-88bb-4096b1ed6a15}
% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = biber
% -*- coding:utf-8 mod:LaTeX -*-


% vv  scroll down to line 200 for content  vv


\let\ifdeutsch\iffalse

\let\ifenglish\iftrue


\input{pre-documentclass}
\documentclass[
  %
  %ngerman, %%% Add if you write in German.
  %
  % fontsize=11pt is the standard
  a4paper,  % Standard format - only KOMAScript uses paper=a4 - https://tex.stackexchange.com/a/61044/9075
  twoside,  % we are optimizing for both screen and two-side printing. So the page numbers will jump, but the content is configured to stay in the middle (by using the geometry package)
  bibliography=totoc,
  %               idxtotoc,   %Index ins Inhaltsverzeichnis
  %               liststotoc, %List of X ins Inhaltsverzeichnis, mit liststotocnumbered werden die Abbildungsverzeichnisse nummeriert
  headsepline,
  cleardoublepage=empty,
  parskip=half,
  %               draft    % um zu sehen, wo noch nachgebessert werden muss - wichtig, da Bindungskorrektur mit drin
  draft=false
]{scrbook}
\input{config}


\usepackage[
  title={Trustworthiness of Synthetic Media in the Context of a Newscast}, % Do not forget to capitalize your title correctly, you may use the following page to help you: https://capitalizemytitle.com/
  author={Danilo Pejakovic},
  %orcid=0000-0000-0000-0000, % get your own ORCID via https://orcid.org/
  email={danilo.pejakovic@campus.lmu.de},
  type={Masterthesis},
  institute={Institute for Informatics}, % or other institute names - or just a plain string using {Demo\\Demo...}
  course={Mediainformatics},
  examiner={Prof.\ Dr.\ Sylvia Rothe, Christoph Weber},
  supervisor={Prof.\ Dr.\ Sylvia Rothe},
  startdate={October 4, 2023},
  enddate={April 4, 2024},
  % Falls keine Lizenz gewünscht wird bitte auf "none" setzen
  % Die Lizenz erlaubt es zu nichtkommerziellen Zwecken die Arbeit zu
  % vervielfältigen und Kopien zu machen. Dabei muss aber immer der Autor
  % angegeben werden. Eine kommerzielle Verwertung ist für den Autor
  % weiter möglich.
  copyright=ccbysa, % ccbysa, ccbynosa, cc0, none
  language=english
]{lmu-thesis-cover}

\input{acronyms}

\geometry{
  left=2.5cm,
  right=3.5cm,
  top=2cm,
  bottom=2cm
}

\makeindex

\begin{document}

\frontmatter
\pagenumbering{roman} % Seitennummerierung mit römischen Ziffern für den Vorspann
\setcounter{tocdepth}{2} % bis zur dritten Gliederungsebene Anzeigen



%tex4ht-Konvertierung verschönern
\iftex4ht
  % tell tex4ht to create picures also for formulas starting with '$'
  % WARNING: a tex4ht run now takes forever!
  \Configure{$}{\PicMath}{\EndPicMath}{}
  %$ % <- syntax highlighting fix for emacs
  \Css{body {text-align:justify;}}

  %conversion of .pdf to .png
  \Configure{graphics*}
  {pdf}
  {\Needs{"convert \csname Gin@base\endcsname.pdf
      \csname Gin@base\endcsname.png"}%
    \Picture[pict]{\csname Gin@base\endcsname.png}%
  }
\fi

%\VerbatimFootnotes %verbatim text in Fußnoten erlauben. Geht normalerweise nicht.

\input{commands}
%\pagenumbering{arabic}
\Coverpage
\Copyright
%Eigener Seitenstil fuer die Kurzfassung und das Inhaltsverzeichnis
\deftriplepagestyle{preamble}{}{}{}{}{}{\pagemark}
%Doku zu deftriplepagestyle: scrguide.pdf
\pagestyle{preamble}
\renewcommand*{\chapterpagestyle}{preamble}



%Kurzfassung / abstract
%auch im Stil vom Inhaltsverzeichnis

\section*{Abstract}

This work addresses the emerging and complex problem of understanding trustworthiness in synthetic media, a domain rapidly evolving with advancements in \gls{ai} and deep learning technologies. In a media landscape increasingly populated with AI-generated content, discerning the factors that influence public trust in such media is paramount. The specific problem tackled in this research is the impact of artificiality, content variation, and the disclosure of \gls{ai} involvement on the perceived trustworthiness of synthetic media.

The study embarked on a comprehensive exploratory analysis, employing various forms of synthetic media, including \gls{tts}, \gls{v2v} conversion, and \gls{ue} generated visuals, to examine how different levels of artificiality and the presence of an AI logo influence viewer trust. A diverse sample of participants (N = 195) was exposed to these media forms in a structured experimental setup, with their perceptions of trustworthiness, realism, and authenticity systematically gauged through a carefully designed questionnaire.

The findings reveal a significant correlation between the degree of artificiality in synthetic media and its perceived trustworthiness, with higher artificiality correlating with lower trust. The study also highlights that viewer perceptions are more influenced by the content and quality of the media than by disclaimers or labels indicating AI involvement. Notably, demographic factors such as age, education, or profession did not reveal significant variations, most likely due to the small sample size.

The study concludes that the rapid advancements in synthetic media necessitate ongoing research to keep pace with the evolving dynamics of trust and credibility in AI-generated content. It advocates for future research to employ more focused, isolated studies and between-subjects designs to delve deeper into specific aspects influencing trust in synthetic media.

\cleardoublepage


% BEGIN: Verzeichnisse

\iftex4ht
\else
  \microtypesetup{protrusion=false}
\fi

%%%
% Literaturverzeichnis ins TOC mit aufnehmen, aber nur wenn nichts anderes mehr hilft!
% \addcontentsline{toc}{chapter}{Literaturverzeichnis}
%
% oder zB
%\addcontentsline{toc}{section}{Abkürzungsverzeichnis}
%
%%%

%Produce table of contents
%
%In case you have trouble with headings reaching into the page numbers, enable the following three lines.
%Hint by http://golatex.de/inhaltsverzeichnis-schreibt-ueber-rand-t3106.html
%
%\makeatletter
%\renewcommand{\@pnumwidth}{2em}
%\makeatother
%
\tableofcontents

% Bei einem ungünstigen Seitenumbruch im Inhaltsverzeichnis, kann dieser mit
% \addtocontents{toc}{\protect\newpage}
% an der passenden Stelle im Fließtext erzwungen werden.

\listoffigures
\listoftables


% Control List of Listings
\let\iflistings\iffalse
%Wird nur bei Verwendung von der lstlisting-Umgebung mit dem "caption"-Parameter benoetigt
%\lstlistoflistings
%ansonsten:
\iflistings
  \ifdeutsch
    \listof{Listing}{Verzeichnis der Listings}
  \else
    \listof{Listing}{List of Listings}
  \fi
\fi

% Control List of Algorithms
\let\ifalgorithms\iffalse
\ifalgorithms
  %mittels \newfloat wurde die Algorithmus-Gleitumgebung definiert.
  %Mit folgendem Befehl werden alle floats dieses Typs ausgegeben
  \ifdeutsch
    \listof{Algorithmus}{Verzeichnis der Algorithmen}
  \else
    \listof{Algorithmus}{List of Algorithms}
  \fi
  %\listofalgorithms %Ist nur für Algorithmen, die mittels \begin{algorithm} umschlossen werden, nötig
\fi

% Control Glossary
\let\ifglossary\iftrue
\ifglossary
  \printnoidxglossaries
\fi

\iftex4ht
\else
  %Optischen Randausgleich und Grauwertkorrektur wieder aktivieren
  \microtypesetup{protrusion=true}
\fi

% END: Verzeichnisse


% Headline and footline
\renewcommand*{\chapterpagestyle}{scrplain}
\pagestyle{scrheadings}
\pagestyle{scrheadings}
\ihead[]{}
\chead[]{}
\ohead[]{\headmark}
\cfoot[]{}
\ofoot[\usekomafont{pagenumber}\thepage]{\usekomafont{pagenumber}\thepage}
\ifoot[]{}


%% vv  scroll down for content  vv %%

\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Main content starts here
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Introduction}
\label{chap:introduction}

%\todo{P1.1. What is the large scope of the problem?}
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{./graphics/scheider-real.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{./graphics/scheider-sd.png}
  \end{subfigure}
  \caption{Stefan Scheider, Anchorman of BR24: real and synthetic Adaptation}
  \label{fig:scheider-real-sd}
\end{figure}
\begin{quotation}
"Sophiscitcated AI systems are increasingly everywhere. [...] However, 2023 will likely prove to be a particularly critical moment in the history of AI" \cite{arguedasAutomatingDemocracyGenerative2023}.
\end{quotation}
This paper and the corresponding study were conducted in the year 2023. As the previously quoted authors state, we might be experiencing a tipping point in \gls{ai} development as more and more tools become available to a broader user base. These developments are tightly linked to the rise of OpenAI's ChatGPT and other widely adopted technologies like \gls{sd} based \gls{t2i} generators. An output example of how future media could be produced is depicted in Figure \ref{fig:scheider-real-sd}. A more detailed description of the relevant technologies will be provided in Section \ref{chap:background} \nameref{chap:background}. \\
Current \gls{ai} tools are often referred to as \gls{genai}: "Generative AI is an umbrella term used for AI systems that can generate new forms of data, often by applying machine learning to large quantities of training data" \cite{arguedasAutomatingDemocracyGenerative2023}. One could extend this definition with the following: Besides just generating new forms of data, \gls{genai} can be used to augment, reduce, manipulate, and mix real data with the generated data in such a form that it is impossible to distinguish between real, syntheticly generated (fake) data, or anything inside that spectrum.

%\todo{P1.2. What is the specific problem?}
In the context of media production and media distribution, the developments of \gls{genai} open up an important discussion about trust and credibility. Media producers have always been using synthetic content for various purposes. One can just think of animated explainatory videos or other infographics. The difference is that most illustrations appeared quite clearly artificial. This has now changed, as generated images and videos can look perfectly authentic and real. At the same time, these technologies are open to being used by anyone, sparking a fear of fake news. Therefore, the question for legitimate media outlets remains whether and how synthetic content will be received among their audiences. Additionally, the term "AI" itself often sparks criticism, due to various reasons. These effects on audiences, their mitigation, and, at the same time, the education of the broader public about technological advancements are very interesting topics for media producers and outlets. Since the developments are quite recent, scarce research has been done so far.

% Second Paragraph
% CORE MESSAGE OF THIS PARAGRAPH:
%\todo{P2.1. The second paragraph should be about what have others been doing}
For the sake of completeness, these discussions are not entirely new. So-called deepfakes(blend word of Deep learning and Fake News) have been around for quite some time. First papers like Facebook's 2014 DeepFace \cite{taigmanDeepFaceClosingGap2014} or the Face2Face approach by \citet{thiesFace2FaceRealtimeFace2020} date back to the year 2016. It took some time until the research gained traction among a broader audience, but at the latest in 2017, in the form of deepfake pornography or revenge porn, deep fakeshit the broader public \cite{coleAIAssistedFakePorn2017}. Or as \citeauthor{westerlundEmergenceDeepfakeTechnology2019a} puts it: "After the introduction of celebrity porn deepfakes to Reddit by one user in late 2017, it only took a few months for a newly founded deepfake hobbyist community to reach 90,000 members" \cite{westerlundEmergenceDeepfakeTechnology2019a}. \\
Quickly afterwards, discussions arose about the implications of these technologies in regards to the spread of fake news. It took some time until the fears in the domain of politics came true. In the meantime, Deepfakes remained problematically present in pornography and, on the positive side, in entertainment and educational content: Jordan Peele faked Obama (2018 \cite{vincentWatchJordanPeele2018}), Channel 4 emitted a fake Queen Elizabeth (2020 \cite{DeepfakeQueenDeliver2020}) and VFX artist Chris Ume went viral with Tom Cruise fakes (2021 \cite{vincentTomCruiseDeepfake2021}).

Despite the proliferation of paid and \gls{oss} solutions for face swaps like \gls{dfl} (2019) or the InsightFace Inswapper (2023), there are only a few known cases where this technology has been used in a particularly influential disinformation video with larger consequences. However, it goes without saying that the effect on social networks under the radar of public control might be much bigger. Some studies about these hypotheses will be featured in Section \ref{chap:rel-work}. \\
The first mention of a trust-dissolving deepfake happened in 2022: fake news of President Zelensky surfaced (Figure \ref{fig:zelensky-Deepfake}), demanding soldiers to lay down their weapons. Although the Russian creators of the video later disputed their creation as "satire" this was certainly not clear within the original video. What had been feared for quite some time had become reality. The technology had been used in a political context, probably for the first time in history, as well as in an armed conflict. Some might say it had been weaponized. Although the video seemed to have no concrete consequences for the soldiers, its role in psychological warfare can't be unseen.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{./graphics/Zelensky.jpg}
  \caption{Left: Deepfake of President Zelensky; Right: real Image of Zelensky \cite{universityofvirginiaZelenskyySurrenderHoax2022}}
  \label{fig:zelensky-Deepfake}
\end{figure}
%\todo{P2.2. Why is the problem important? Why was this work carried out?}
Progressing in time, in the second half of 2022 several things changed in the space of \gls{ai} tools.

The aforementioned \textit{simple} face swaps are now in good company in an ever-growing toolbox of \gls{ai} services: 
\begin{enumerate}
  \item GPT-enabled Chat Applications were released at the end of 2022
  \item \gls{t2i} generation was released in summer 2022
  \item Excellent AI voices cloning tools became available during 2023
\end{enumerate}
While ChatGPT is probably less relevant in the audio-visual domain, it still fuels much of the public opinion about \gls{ai} tools, as it is probably the best-known and fastest-growing software tool of all time. The other items on the list have drastically improved the quality and possibilities of how and what kind of synthetic media can be created. In recent months, there have been several reports about their use with increasing frequency. The examples of fake voices and face swaps on social media in 2023 are innumerable and can be traced back to the availability of online services such as Resemble.ai or Elevenlabs. \\
This also leads to several nefarious use cases: To name some examples in the German context, in September 2023 a primetime news host was recreated with a fake voice in order to advertise dubious financial products (Figure \ref{fig:sievers-fake}). By using Elevenlabs' checking tool, one can quickly tell that the voice was likely created with their software.(figure: \ref{fig:sievers-11labs}). \\
At the end of November 2023 \textit{two} videos of German chancellor Olaf Scholz were released. One was part of a commercial campaign for a German yellow press newspaper \cite{dwdl.deSpringerTrommeltMit}. The second is part of an art or protest project \cite{zdfKunstinstallationDeepfakeScholzVerkuendet}. Example images for these cases are not included, as they don't make any sense without the faked audio. 

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{./graphics/sievers.png}
    \caption{fake of Christian Sievers \cite{zdfDeepfakeMitZDFModerator}}
    \label{fig:sievers-fake}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{./graphics/sievers-11labs.png}
    \caption{Elevenlabs audio analysis \cite{elevenlabsAISpeechClassifier}}
    \label{fig:sievers-11labs}
  \end{subfigure}
  \caption{Christian Sievers Deepfake and Elevenlabs Audio Analysis}
\end{figure}

It is only logical to expect a further increasing frequency of such content, both in the case of legitimate (commercial, education, entertainment) and illegitimate (scam, disinformation). An environment where both categories of content coexist is very challenging in terms of trust. For legitimate newsmakers, the question arises: how can it combat disinformation and, at the same time, use the advancements of \gls{genai} to improve production workflows? This is a dilemma that has not been solved yet. \\
The effects of these very recent technical capabilities have not yet been studied, which is why this work attempts to do so. In a time where the very existence of \gls{genai} raises trust issues with every kind of content, specifically those synthetically generated, any findings about synthetic media reception might be helpful in better addressing all the named issues.

% Third Paragraph
% CORE MESSAGE OF THIS PARAGRAPH:
%\todo{P3.1. What have you done?}
This paper tried to explore the effect on potential recipients of synthetically created or AI-enhanced media in the specific context of a Bavarian/German \gls{psm} news show, namely \textbf{BR24}. To be more specific, the focus of the research questions were: 

\begin{enumerate}
  \item Trust and credibility in media with varying degrees of artificiality.
  \item Effects of AI disclosure by placing an AI logo watermark on the material.
  \item Are there differences within audience subgroups (age, occupation, etc.)?
  \item Are there differences based on the used display size?
  \item Finding the most significant of the aforementioned variables.
\end{enumerate}

To approach these questions, an online questionnaire was conducted. The time window was 40 days, from the 6\textsuperscript{th} of November 2023 until the 15\textsuperscript{th} of December 2023. During this timeframe, 195 valid answers were counted. The details and results will be described in Section \ref{chap:study}. \\
Before conducting such a study, the content itself needed to be created to ensure a high degree of control over certain variables. To accomplish this, several workflows had to be established, which included several experiments with various \gls{oss} tools and how they could work together. In addition to the AI software, traditional video editing tools like \gls{prpro} and \gls{ae} were used to finalize the videos. A detailed process description of how the material has been created will be described in Chapter \ref{chap:implementation}. The authors conducted this work in association with the German public broadcaster \gls{br} and the \gls{hff}. No conflicts of interest can be reported in this constellation.

%\todo{P3.2. What is new about your work?}
The methodology sought to conduct a study about trustworthiness with various AI-generated or assisted videos.
The tested videos were carefully designed by taking into account an extensive toolchain of available open source technology, making it (theoretically) possible for every media producer to recreate similar results, implement (semi)automatic workflows for their media production, and conduct further experiments. \\
As the tools are very recent developments, to our knowledge, no comparable studies have been conducted yet.

% Fourth paragraph
% CORE MESSAGE OF THIS PARAGRAPH:
The study revealed several findings regarding the relationship between synthetic media's artificiality and perceived trustworthiness. A clear correlation was identified: as artificiality increased, trustworthiness decreased. Contrary to expectations, the presence of an AI logo did not significantly impact trust, suggesting that the content's inherent qualities and presentation were crucial in shaping viewer perceptions. \\
Regarding demographics, the lack of significant differences in trust perceptions across various groups suggests that while there may not be a 'universal pattern,' the influence of demographics on trustworthiness perceptions in synthetic media might be more nuanced or less pronounced than anticipated. This calls for a more granular exploration of demographic influences in future research. \\
These findings have profound implications for the production and dissemination of synthetic media. They underscore the need for media creators to prioritize realism and high-quality content to maintain viewer trust. Moreover, the minimal impact of AI logo disclosures on trustworthiness perceptions prompts a reevaluation of transparency practices in media production. In a broader context, this study contributes to the critical discourse on the ethical use of AI in media, highlighting the importance of ongoing research and dialogue to navigate the challenges posed by synthetic media technologies.
\chapter{Background}
\label{chap:background}
This work focuses on the social aspects of synthetic media consumption. Because of that, we decided not to mention deep details of the technological side of machine learning in Chapter \ref{chap:rel-work}, \nameref{chap:rel-work}. However, for the study and accompanying videos, plenty of \gls{ai} technologies have been implemented. To aid the overall understanding of the whole paper, some technological background will be laid out in this section.

\begin{quotation}
"Although it is difficult to pinpoint, the roots of AI can probably be traced back to the 1940s, specifically 1942, when the American Science Fiction writer Isaac Asimov published his short story \textit{Runaround}" \cite*{haenleinBriefHistoryArtificial2019}. 
\end{quotation}

Besides science fiction, the practical science was evolving during wartime. After Alan Turing famously engineered a computer to crack the Enigma cryptography, he published his seminar article "Computer Machinery and Intelligence" where he described how to create intelligent machines and, in particular, how to test their intelligence \cite{haenleinBriefHistoryArtificial2019}. In the following years, the term "artificial intelligence" rose to more prominence, most notably at Dartmouth College, where Marvin Minsky and John McCarthy hosted the \textit{Dartmouth Summer Research Project on Artificial Intelligence (DSRPAI)} in 1956 \cite{flasinskiHistoryArtificialIntelligence2016}. 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/Timeline_of_generative_models_by_type.png}
  \caption{Timeline of generative Models by Type. \citet{garcia-penalvoWhatWeMean2023}}
  \label{fig:timeline-models}
\end{figure}

The different times when \gls{ai} had its highs and lows are often refered to as the Four Seasons of AI. Spring, representing the dawn of \gls{ai}, was followed by the summer. After the events at Dartmouth College, a lot of funding from US institutions such as DARPA or the RAND Corporation went into AI research. Without going to much into the details of various developments, one can state that this first hype abruptly ended around 1973 when the high governmental spending was cut. The first \gls{ai} winter is often credited to Marvin Minsky and Seymour Papert, who published their famous book “Perceptrons” \cite{minskyPerceptronsIntroductionComputational2017} in 1969, in which they showed the strong limitations of perceptrons, e.g., the inability to compute some logical functions like XOR. As a result, many \gls{ai} researchers concluded that the study of neural networks was not promising \cite{flasinskiHistoryArtificialIntelligence2016}. \\
\Citeauthor{haenleinBriefHistoryArtificial2019} stated that, although the Japanese government began to heavily fund \gls{ai} research in the 1980s, to which the U.S. DARPA responded with a funding increase as well, no further advances were made in the following years. This can only be partially held true because some progress had to be made before the second \gls{ai} summer came around, notably multi-layer perceptrons and therefore deep neural networks in 1965, backpropagation in 1970, convolutional neural networks in 1979, autoencoders in 1986, generative adversarial networks in 1990, to name just a few \cite{schmidhuberAnnotatedHistoryModern2022}. \\

The current \gls{ai} summer arrived with \citet{krizhevskyImageNetClassificationDeep2012} and their significant advancements in image recognition using a convolutional neural network, "AlexNet". Their network performed considerably better than the previous state-of-the-art. In 2015, AlphaGo became the first \gls{ai} to beat Grandmasters in the game Go. \\
Besides the image recognition domain, text and natural language processing received huge performance improvements with \citetitle{vaswaniAttentionAllYou2023} and their "Transformer" architecture in 2017. \gls{tts} also benefited from transformer research, with major advancements in \citetitle{wangTacotronEndtoEndSpeech2017} in 2017 \cite{wangTacotronEndtoEndSpeech2017}. And to name one of the most recent advancements, diffusion-based approaches are to be mentioned, which gave the powerful \gls{t2i} generator Stable Diffusion its name. \cite{rombachHighResolutionImageSynthesis2022}. A chronological overview of the model developments can be depicted in Figure \ref{fig:timeline-models}. \\
As scientific advancements are numerous, so are their practical implementations. Those relevant to this work will be briefly described below.

\section{Generative AI}
\label{sec:genai}
The term \gls{genai} has been briefly mentioned in the introduction, but for better understanding, the term shall be examined deeper to avoid misunderstandings and ambiguity. There is no globally agreed definition for "Generative AI" \cite{garcia-penalvoWhatWeMean2023}. \\
In technical terms, a generative model, as described with a \gls{gan}, refers to a specific subform of neural network architecture. These are differentiated from discriminative models by their internal processes and the probabilities they estimate \cite{garcia-penalvoWhatWeMean2023} in \cite{gmComprehensiveSurveyAnalysis2020}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/gtrends_genAI_1712-2312.png}
  \caption{Google Trends of "Generative AI" from December 2017 to December 2023 \cite{googletrendsGoogleTrendsQuery}}
  \label{fig:gtrend-genai}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/gtrends_deepfake_1712-2312.png}
  \caption{Google Trends of "Deepfakes" from December 2017 to December 2023 \cite{googletrendsGoogleTrendsQuerya}}
  \label{fig:gtrend-deepfakes}
\end{figure}

It is unlikely that the broader public refers to the same, rather technical context. It is more likely that the meaning is less about technical implementations but more about how the end user utilizes the software. If things can be generated using \gls{ai} It's got to be named \textit{generative} \gls{ai}. This notion can be supported by Google trends for "generative AI" as depicted in Figure \ref{fig:gtrend-genai}. The search requests begin to rise in October 2023 and then climb high from December 2023 onwards. This fits perfectly with the release of ChatGPT and the spread of image generation software and accompanying media coverage. \\
In the following, the term "\gls{genai}" will be used in the understanding of the broader public and not the narrow technical definition. If technical details are to be discussed, they will be elaborated further. 

\section{Uncanny Valley}
Besides the technical terms, we have to quickly cover the uncanny valley as well. The concept of the uncanny valley was first described by Masahiro Mori, a robotics professor at the Tokyo Institute of Technology. He hypothesized that a person's response to a humanlike robot would abruptly shift from empathy to revulsion as it approached, but failed to attain, a lifelike appearance \cite{moriUncannyValleyField2012}. Some examples of this spectrum are given in Figure \ref{fig:uncanny-valley}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/uncanny-valley.png}
  \caption{Uncanny Valley according to Masahiro Mori \cite{moriUncannyValleyField2012}}
  \label{fig:uncanny-valley}
\end{figure}

Knowledge about the Uncanny Valley is important because its concept can be widely adopted to other domains besides robotics. We took its effect into account for each test case within our study. As we investigate trust in \gls{ai}-aided news, we have to consider that a loss in trust could also be related to uncanny valley effects. \\
We will address how we handled the effect of the uncanny valley in our study design in Section \ref{sec:study design}.

\section{LLM-based Chatbots}
A LLM chatbot (ChatGPT) has been used at several points to conduct this research. Especially during the software development phase(Chapter \ref{chap:implementation}) and the study design (Section \ref{chap:study}) ChatGPT has been consulted to speed up the processes massively. \\ 
For the content generation of our study videos, it did not play any role and thus won't be discussed as deeply as the other technologies. However, we must consider the mere existence of ChatGPT in the context of the time of writing this work. As discussed in Section \ref{sec:genai} the public attention towards AI tools has been tremendously accelerated by the broad availability of ChatGPT. When compared to the slow and steady increase of the Google trends about deep fakes in Figure \ref{fig:gtrend-deepfakes} we can clearly see a more disruptive tendency on the timeline of ChatGPT. \\
As a standing fact, we have to consider how an individual's \gls{ai} education could influence his or her response to the study.

\section{Face-Swapping}
\label{sec:face-swapping}
Similar to the previous section about ChatGPT, face swaps have not been used directly within this project to create media but have played a very important role in shaping public opinion about synthetic media. As mentioned in the introduction, these face swaps initially gave synthetic media the name \textit{deepfake}, which is being used today. This fact often shifts synthetic media toward having a negative connotation. \\
Besides an alerting problem with deepfake pornography and revenge porn, the first publicly recognized deep fakes were those of Obama/Jordan Peele face swaps in 2018 \cite{vincentWatchJordanPeele2018}. \citet{hancockSocialImpactDeepfakes2021} stated that the Obama video is likely the most canonical, if not the original "Deepfake" video. \\
Comparing it with Figure \ref{fig:gtrend-deepfakes} this also fits into the timeline with the Google trend search. These early deep fakes are also responsible for the few research papers being conducted on that topic. They will be featured in Chapter \ref{chap:rel-work} about the related work. Therefore, this section will focus only on the technical side of face swaps.

Prior to this paper, extensive experiments have been conducted with different face swap toolkits. In the end, no face swaps have been performed for the study examples. The reason for this decision lies in the study design: Briefly captured, a controlled environment was needed in order to rule out as many disruptive factors as possible. The TV-News setup was chosen as it provided a steady setting. In this case, face swaps were not needed. They could have been optionally used to improve the final quality of the rendered faces, but due to time constraints, this approach was not carried out. The idea will be picked up again in Section \ref{sec:lips} when the quality of lip-remapping is addressed.
Still, the findings about face swaps are interesting in the background context of this work and thus will be included shortly in the following paragraphs.

\subsection{DeepFaceLab}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/dfl-demo.png}
  \caption{DeepFaceLab Test Result from September 2021}
  \label{fig:dfl-sample}
\end{figure}
By far the leading software for face-swapping is DeepFaceLab, or DFL for short. It is also one of the first and oldest in the growing line of synthetic media creation tools. It encompasses a broad workflow in order to create high-end face models. According to the commit history, \gls{dfl} came into existence in June 2018 \cite{iperovCommitsIperovDeepFaceLab}. Without being able to say it with certainty, DFL's success might also lie in its proximity to pornography. The reason for this suspicion is that the main guide for how to work with \gls{dfl} is hosted on a subpage of \textit{mrdeepfakes.com}, which claims to be the largest deepfake porn site. To our knowledge, these topics were rarely addressed by the authors of the software.\\
As of November 2023 the software has been phased out of development and archived by the lead developer without providing any reasons. Probably this had to with the upcoming release of newer, faster methods of face-swapping discussed in Section \ref{sec:roop}. For the high-end workflow, \gls{dfl} can still be used, but there are also other alternatives. \textit{FaceSwap}, actually the first tool introduced in 2017, is under development. The standard workflow for creating a DFL model is as follows:

The terms \textit{\gls{dfl-dst}} and \textit{\gls{dfl-src}} are to be understood in such a way that the \gls{dfl-dst} is the face that will be driving the final face. The \gls{dfl-src} is the face that is being faked onto the \gls{dfl-dst}. \\
\textbf{1) data gathering} involves finding the right material to train the models with. High-resolution images of both the \gls{dfl-dst} and the \gls{dfl-src} are required but can be easily found online in videos. The final face resolution usually ranges from 128x128 up to 512x512 pixels. The videos used for data gathering should be of such quality that the required face size can be extracted from the gathered videos. It is very important that the images cover a wide variety of facial angles, expressions, and lighting situations. Usually, around 8.000 face images are enough for a good fake. \\
During \textbf{2) face extraction}, faces of the desired size are detected in video frames and extracted. Also, facial alignment data gets embedded into the images as metadata, as it is required for the training process. \\
Afterwards, \textbf{3) sorting and data refinement} are needed to ensure that face alignments have been correctly identified. This step also involves sorting out unwanted images. Exclusion criteria include blurriness or wrongly detected faces, for example. \\
Next, \textbf{4) mask segmentation} comes into play. Here, the face gets masked out by a tool. This involves drawing manual face masks to fit the \gls{dfl-dst}s and \gls{dfl-src}s face geometry needed for easier merging later on. In this, obstructions in front of the face can be masked so that the model learns to mask them out as well during merging.  This concludes the pre-processing in most cases.\\
Afterwards, the \textbf{5) training} can begin. Training can take, depending on hardware, resolution, and dataset size, some days up to weeks or months to conclude. The training itself is divided into multiple stages where different hyperparameters have to be adjusted. After training, the final \gls{dfl-dst} can be \textbf{6) merged} with the synthetically generated face. One example of the generated quality can be seen in Figure \ref{fig:dfl-sample}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/df-model-arch.png}
  \caption{"DF" Model Architecture Diagram \cite{perovDeepFaceLabIntegratedFlexible2021}}
  \label{fig:df-model-diagram}
\end{figure}

\gls{dfl} face swaps are based on an autoencoder architecture like that described in Figure \ref{fig:df-model-diagram} though in the meantime, several subvariants have developed. Autoencoders are notoriously known for their inability to create sharp images. Because of that, face-upscaling GANs are added to the training process. \\
As can be seen, the usage of \gls{dfl} is quite complex and time-consuming. On the other side, it provides a great degree of freedom compared to newer and simpler methods.

\subsection{Inswapper}
\label{sec:roop}
Developed on the foundations of the InsightFace Face Analysis Project \cite{insightfaceInsightFaceWebsite} \textit{Inswapper} provides a very easy way to swap faces without the need for any training before inference. The workflow is as simple as loading a targeted video into the GUI of \textit{roop} \cite{sangwanRoop2023} as well as a source image, and waiting for the software to create the final video. The model used in the background is closed-source and thus cannot be retrained. Besides the enormous acceleration of the whole process, the lack of customization is the biggest drawback in comparison to \gls{dfl}. Overall, the quality of the generated Inswapper faces is quite good. The quality of the generated faces drops significantly in situations where the face is obscured or angled in profile towards the camera. Some examples are included in the Appendix \ref{chap:insightface-demos}. \\
Although it was not stated specifically by the \gls{dfl} developers, such rapid advancements in face swap technology might be the reason why \gls{dfl} was discontinued.

\section{Image Generators}
\label{sec:stable-diffusion-bg}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/latent-diffusion.png}
  \caption{Latent Diffusion Model \cite{rombachHighResolutionImageSynthesis2022}}
  \label{fig:ldm-arch}
\end{figure}

\citet{rombachHighResolutionImageSynthesis2022} released their \gls{ldm} (Figure \ref{fig:ldm-arch}) in the summer of 2022 and started a huge movement in computer graphics. We won't go into further details about the inner workings of the \gls{ldm} workflow, but a brief overview is given in Appendix \ref{app:diff-workflow}.\\
The open-sourced workflow has been quickly adopted in multiple products and fuels several companies such as \textit{Midjourney, Pika-Labs, Runway, DI-D}, and many more. Even more interesting than the appearance of commercial solutions is the fact of how quickly a huge open-source community grew from the \gls{sd} project. Currently, there are around a dozen different GUI projects available for \gls{sd}, some even featuring extensions within the GUI. To name the biggest, \textit{\gls{auto1}} lists \textbf{269} community extensions that massively extend the functionality in different directions, for example, towards video generation. The project was initiated in August 2022, has 22.500 forks and a 113.000 star rating, which is an impressive rate of development \cite{AUTOMATIC1111StablediffusionwebuiStable}. \\
Midjourney, probably the best-known commercial solution, works over a Discord chatbot interface and currently has around \textbf{17.3} million registered users on their channel \cite{midjourneyJoinMidjourneyDiscord}.

Besides the development of image generator software, there is also a thriving community for model development and fine-tuning so-called \gls{lora} models. These are then shared on platforms such as \textit{\gls{hf}} or, more prominently, \textit{civitai.com}. While \gls{hf} is focused on the developer side of models, civitai.com can also be seen as some sort of social network where user-generated content in the form of generated images is showcased. Civit hosts "thousands of high-quality Stable Diffusion models" as they currently state on their website \cite{CivitaiHomeOpenSource}. \gls{lora}s are especially interesting as they are very small (several megabytes) compared to full-fledged models (several gigabytes). LoRas help in fine-tuning the bigger models towards a specific goal. Because LoRas are so small, they can be shared very easily within the community. It goes without saying that this sort of plug-and-play model is also well suited and used for pornographic purposes as well. Different from the earlier mention of \textit{mrdeepfakes.com}, \textit{civitai.com} features nudity filters and seeks to create a safer space for every kind of content. A public debate has already started about whether AI-generated pornography will replace pornography one day. Surely the ethical concerns could fill a paper on their own and thus won't be discussed much further. However, some will be mentioned as related work in Chapter \ref{chap:rel-work}.

Latent Diffusion models act not only as the core technology but also as a platform, and the adoption speed and extensiability of both the technology and the community indicate a disruption is taking place here. \\
In the context of this paper, a stable diffusion workflow has been used to generate test videos in the style of computer animation (Figure \ref{fig:scheider-real-sd}). The used workflow will be featured later in Section \ref{sec:sd-video}. \\
In the meantime, \textit{stable video diffusion} has been released. This showcases the rapid rate of advancement in the field of \gls{ldm}s: In just one year text-to-image generators proceeded to become text-to-video generators as well.


\section{Lip-Remapping}
\label{sec:lips}
After discarding the previously discussed whole-face swaps using \gls{dfl} as a meaningful tool for the study, \textit{lip-remapping} was chosen as one of the main visual technologies for this work. The basis for many videos were real recordings from a news show. After creating a synthetic voice (refer to sections \ref{sec:tts} and \ref{sec:v2v}) the proper mouth movement needed to be recreated to make the videos somewhat convincing. \\
To accomplish a satisfying result, multiple tools needed to be chained together. First and foremost, there is \textbf{Wav2Lip (\gls{w2l})}. Based on the research of \citet{prajwalLipSyncExpert2020} they provide a code implementation on GitHub \cite{mukhopadhyayWav2LipAccuratelyLipsyncing2023}. The code was adapted to fit the workflows for the creation of the study videos and explained in further detail in the implementation Chapter \ref{chap:implementation}. The \gls{w2l} project is also one of the older synthetic media tools, released in 2020.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/w2l-arch.png}
  \caption{Wav2Lip Architecture}
  \label{fig:wav2lip-arch}
\end{figure}

The architecture (Figure \ref{fig:wav2lip-arch}) is based on a GAN generator-discriminator approach, adding an additional "Lip-Sync Expert" discriminator which improved the results significantly in comparison to previous methods. \\
Unfortunately, the available public \gls{w2l} model has been trained on a face resolution of only 96x96 pixels, which is insufficient for a convincing effect. To address this issue, a face-upsampling GAN was added to the workflow to increase the facial resolution. The upsampling was performed using the public \textbf{GFPGAN} implementation based on the works of \citet{wangNeuralSourcefilterbasedWaveform2019}. \\
A result can be seen in Figure \ref{fig:wav2lip-demo}: To the right, one can see the original actor; the middle shows a \gls{w2l} result where the mouth region is blurry; and the left depicts the GFPGAN upsampling. The GFPGAN workflow splits the blurry video into individual frames and upsamples each face individually. This introduces flickering artifacts in the face due to small inconsistencies after upsampling that look like flickering once played back with 25 \gls{fps}. This issue can be mitigated with further post-production.

Overall, the process of creating the lip-remapping is rather slow. One 20-second video takes over 5 minutes to export. The additional post-production and compositing of all audio and video sources back together takes around 20 minutes after the workflow is repeated multiple times. \\
Using GFPGAN was not the only option to improve the face quality. As mentioned in Section \ref{sec:face-swapping} we could have used \textbf{\gls{dfl}} on top of the \gls{w2l} output to sharpen the result with a full face swap. The method has been tested on other occasions and works quite well. It also does not have issues with face flickering. But on the the other hand, creating a \gls{dfl} model is very time-consuming, especially if it needs to be good. The processes within this work needed to be somewhat practical for everyday use in a media company. If every lip-remapped actor needs its own \gls{dfl} model trained, this is not practical. One-image-swap solutions, like the previously described Inswapper, does not improve the quality of the \gls{w2l} outputs. \\
Ideally, one would work with a higher-resolution \gls{w2l} model or newer approaches like those from \citet{guptaGeneratingUltraHighResolution2023}. The authors state that their approach delivers great results at 768x768 pixels compared to \gls{w2l}'s 96x96 resolution. Unfortunately, there was no available code implementation of the paper at the time, so it couldn't be tested.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/wav2lip/wav2lip-demo.png}
  \caption{Lip-Remapping Workflow, described from right to left}
  \label{fig:wav2lip-demo}
\end{figure}

\section{Text-to-Speech}
\label{sec:tts}
To cover the audio component of the videos, \gls{tts} has been used. Although there are plenty of web-based solutions (\textit{elevenlabs.io, resemble.ai}) one goal of the project was to only use \gls{oss} solutions. Regarding \gls*{tts} the decision was made to use \textbf{CoquiTTS}. CoquiTTS is a library for \gls{tts} generation with pretrained models in +1100 languages \cite{erenCoquiTTS2021}. CoquiTTS, which developed out of a Mozilla project, is also community-driven but also provides a \gls{saas} called Coqui-Studio. In comparison to the earlier mentioned stable diffusion community, Coqui's community is not as big, and the community support and documentation is worse. But at least there is some documentation about training a new voice, and it could be done. The results were satisfying but certainly leave plenty of room for improvement compared the state-of-the-art commercial solutions.

One of the biggest challenges in implementing \gls{tts} was the gathering and pre-processing of training data. In exchange with the community, a dataset size of around 4 hours of speech was required to get decent results. The difficulty did not lie in getting the raw data; this was easily obtained from the archives of the \gls{br}. In order to train the \gls{tts} model, the data needed to be structured in a special format: Audio segments needed to be under 10 seconds in length, and each audio file needed accompanying transcription in a text file. To solve these issues on a scale of 4 hours of content, a toolkit has been developed, which will be explained in detail in Section \ref{sec:dvt}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{./graphics/tts/tts-workflow.png}
  \caption{Basic \gls{tts} Workflow. Image Source: \cite{jemineRealTimeVoiceCloning2019}}
  \label{fig:tts-explainer}
\end{figure}

Usually, \gls{tts} is accomplished by using multiple stages of training multiple models. A basic overview of how they work together can be seen in Figure \ref{fig:tts-explainer}. The inputted text gets encoded as a mel spectrogram by the synthesizer. Based on these mel spectrograms, the vocoder then generates an audio file. \\
Most of the architectures implemented by CoquiTTS feature a synthesizer model that converts the input text to a mel spectrogram and a vocoder that translates the spectrogram to an audio file. However, the development in the TTS domain is rapid, and many other methods have been created. \\
CoquiTTS lists several implemented approaches, such as 13 spectrogram models, 5 end-to-end models, 6 attention methods, 2 speaker encoders, and 8 vocoders. These won't be covered in depth, as they are not the focus of this paper. If of any interest, please refer to the CoquiTTS documentation and corresponding papers.\\
For ease of use, we decided to use the VITS end-to-end model for this project, developed by \citet{kimConditionalVariationalAutoencoder2021}. In contrast to the traditional multi-model approaches, VITS offers full TTS capabilities by training just a single model. This approach is faster and more reliable in most cases. A downside of using VITS is that it gives less granular control over the models. Our project favoured speed over quality and opted for the easier workflow of VITS. Naturally, the state of the art changed quickly, so the current approach may soon be outdated.

CoquiTTS also recently added a faster cloning method called X-TTS. It is also an end-to-end model like VITS, but with the addition that it can be used in a zero-shot manner. This means it can reproduce voices it hasn't been trained on, which is achieved by providing a reference voice in addition to the input text. Although we do not know for sure, it probably works similar to the \gls{rtvc} toolkit by \citeauthor{jemineRealTimeVoiceCloning2019}. Most likely, Elevenlabs' "Instant Voice Clone" also works with this technology under the hood.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{./graphics/rtvc.png}
  \caption{RTVC Architecture \cite{jemineRealTimeVoiceCloning2019}}
  \label{fig:rtvc-arch}
\end{figure}

The \gls{rtvc} workflow cleverly introduced a speaker encoder to the training process of the synthesizer (see Figure \ref{fig:rtvc-arch}). The speaker encoder is based on speaker verification and voice recognition and provides a speaker embedding. The synthesizer is then trained with the speaker embeddings as an additional input. During inference, one has to provide a speaker embedding (the reference voice sample). Based on the embedding, the synthesizer then produces the cloned voice without having been trained on the voice in advance. We have been experimenting with the \gls{rtvc} in 2021. At the time, it only worked with English voices, and we tried to create a German model as well. The results were unsatisfying. Today, with X-TTS, the German voices do not sound good enough which is why we switched back to the time-consuming retraining approach with VITS. \\
Regarding our VITS training duration, we first trained a first run with 90 minutes of training material for two weeks of a RTX 4090 GPU. The second run with 4 hours of material was conducted on an A6000 GPU took approximately one week until the loss values converged. 

\section{Voice-to-Voice Conversion}
\label{sec:v2v}
Besides \gls{tts}, another recently published approach should be tested: \gls{v2v} conversion. The main difference is that one does not input text but actual speech into the converter and receives an audio file with the voice of the targeted voice. This approach can address the problem with \gls{tts}, where the synthesized voice sounds monotonous. Because the tonality of the target speaker is retained after converting it to the other voice, an actor can influence how the result should sound. On the other hand, certain mannerisms in the actor's voice will translate to the result. That can be an odd-sounding pronounciation of the letter "S" or "R", hissing, or something else. \\
For accomplishing \gls{v2v} conversion, the \gls{rvc} project was used \cite{RVCProjectRetrievalbasedVoiceConversionWebUI2023}. 

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/RVC-UI.png}
  \caption{RVC Gradio User Interface}
  \label{fig:rvc-gradio}
\end{figure}

The project features a gradio.io user interface (Figure \ref{fig:rvc-gradio}) for easy cloud deployment and is quite self-expalinatory. Regarding the dataset requirements, approximately 10 minutes of a person's voice is required. The data pre-processing does not require any text file transcription as with \gls{tts} but does require cutting up the voice samples in sections under 10 seconds to avoid an \gls{oom}. This can be easily achieved with the pre-processing toolkit developed for the \gls{tts} section of this work. The training itself takes approximately one hour on an A6000 GPU.

After training is completed, inferencing the model is very fast. With \gls{rvc}, it processes inputted audio files in near real-time. It is to be noted that an actual real-time voice changer for these models is implemented as well\cite{WokadaVoicechangerVoice}. One can use input from a file or microphone, and the voice changes with very little delay (100ms on a suitable graphics card). The implications of such real-time voice-fake capabilities are very interesting in the domain of deepfakes and scams. There have been some reports that fake voices have been used to conduct scams. However, these were often accomplished using \gls{tts} and preparing the sound snippets for playback. Combining such a technology with real-time face swaps, hints towards needed caution and multifactorial security in videocall-based social interaction. \\
Interstingly these voice models are shared as among creators on platforms such as Civitai, but the technology has not yet had as much of an impact on broader society as stable diffusion has.

\gls{v2v} models could also be used to improve \gls{tts} quality in a similar fashion as suggested with Wav2Lip and face swaps; One could train a faster but lower quality \gls{tts} model and then upsample the voice quality in realtime with \gls{v2v}. No experiments were conducted in this direction due to time constraints, as that would have required a shift in the \gls{tts} approach.

\section{Game Engines and virtual Production}
\label{sec:bg-virtual-production}
A few years before the big leap in \gls{genai} virtual production methods started to gain traction within the film and broadcasting industry. Instead of filming in front of a greenscreen, \textit{The Mandalorian} (2019) famously used a large LED panel around the whole studio walls and ceiling to shoot their movie (refer to Figure \ref{fig:mando-vps}). That way, many effects could be achieved "in camera" opposed to "in post production" as it is usually done.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/mandalorian-vp.jpg}
  \caption{\textit{The Mandalorian} Studio and LED-Volume "Stagecraft" \cite{landsiedelGamechanger2021}}
  \label{fig:mando-vps}
\end{figure}

The important shift in technology was not only the use of LED panels but also what drives the images on the screens. It is software, usually found in the gaming industry: Unreal Engine (\gls{ue}), a so-called \textbf{game engine}. The engine's capability to render photorealistic images in real time made it the ideal driver for this novel movie production. Today \gls{ue} can be used to generate not only environments, but humans as well. Their software, \textit{Metahuman Creator}, allows for easy character creation. It is also possible to scan a face and transfer the geometry onto a metahuman. An example of such an approach can be seen in Figure \ref{fig:metahuman-comp}, although this particular experiment did not work perfectly well. 

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{./graphics/photogrammetry.png}
    \caption{Photogrammetry Scan}
    \label{fig:head-photogrammetry-scan}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{./graphics/Metahuman.png}
    \caption{Metahuman based on photogrammetry Scan}
    \label{fig:metahuman-result}
  \end{subfigure}
  \caption{Mesh to Metahuman Workflow}
  \label{fig:metahuman-comp}
\end{figure}

Already during the timeframe of the video creation, the process for creating metahumans had improved considerably. It is only a matter of time until these metahumans get more advanced. Currently, gaussian splatting seems to be a key technology to watch in these regards. \\
It has to be noted that \gls{ue} is a complicated tool that needs a lot of time to be mastered. In addition to \gls{ue}, many other, more specialized tools need to be used to get the best possible output. These include modelling tools for environments and characters, as well as texturing tools and many more. \gls{ue}, in the end, is good at combining all the assets, but to get high quality results, a team needs experts in many different areas of traditional computer animation. Especially the metahuman characters fall into the uncanny valley, which makes the results unsuitable for a good study. It was explored to improve the face quality using a \gls{dfl} model on top of the \gls{ue} rendering. Unfortunately, the uncanny part of the face is not the appearance of the face but its movements. This most likely has to do with how the face is animated. The facial performance capture is being accomplished using the iPhone's FaceID camera, which has limitations in how many facial landmarks it can track and therefore reduces face movement fidality. Because of that, the face movements do not look very natural and as the movements would translate to the \gls{dfl} face as well, it would not improve anything.

Even though the quality of the metahuman result was not compelling, they were included  within the study,as will be elaborated on in Section \ref{chap:study}. 

\chapter{Related Work}
\label{chap:rel-work}
% Introduction
While Chapter \ref{chap:background} served the purpose of giving some background on the technical side and establishing a general understanding of how the used technologies can be applied. Now we want to present some related work in the domain of psychological and sociological research, which has been done in the context of (synthetic) media trust and credibility and the effects of fake news in general. 

\begin{quotation}
  "[...] We urge researchers to begin to study the social issues surrounding deepfake technology. The studies in this volume do a fantastic job of mapping out the research questions, applying theory to the phenomenon, and creating new tools to apply to future research. But this study is preliminary, and we urge scholars to build upon this study as deepfake use continues to grow." \cite{hancockSocialImpactDeepfakes2021}
\end{quotation}

With this call to action, "Cyberpsychology, Behavior, and Social Networking" (volume 24, issue 3) published several articles that can be summarized as "the social impact of DeepFakes". This was in 2021, three years after deepfake face swaps have become dominant. To begin with, \citet{hancockSocialImpactDeepfakes2021} raised a few questions about DeepFakes: Does exposure to deepfakes undermine trust in the media? How might deepfakes be used during social interactions? Are there strategies for debunking or countering deepfakes? They concluded that empirical research on the social impact of deepfakes is scarce and therefore referred to neighbouring fields. \\
In an introduction to the topic as a whole, \citet{hancockSocialImpactDeepfakes2021} cite studies on "false memory aquisition" by \citet{garryActuallyPictureWorth2005}. These experiments often involve some doctored footage of participants, often created with the tools of 3D animation, and prove that it is possible to induce false memories with that approach. \\
\citet{hancockSocialImpactDeepfakes2021} also mention deception research and conclude that people perform only slightly above chance when evaluating a message as either true or deceptive \cite{bondAccuracyDeceptionJudgments2006}. To quote: "Studies have shown that deception detection is approximately the same whether the message is conveyed through text (e.g., a court transcript, an Internet chat log), an audio recording (e.g., a voicemail, a radio program), or a video (e.g., an interrogation video)" \cite{hancockSeeNoEvil2010} cited in \cite{hancockSocialImpactDeepfakes2021}.

The references by \citet{hancockSocialImpactDeepfakes2021} show how far research in the field can reach. In the following, we will present further examples. We decided to divide the related work into three categories: In \ref{sec:hist-context} we want to provide some context about the environment in which the papers were written. In \ref{sec:rel-secondary} we shed some light on methods that don't study participants directly in a study but analyze secondary indicators such as newspaper articles, forum blog posts and video comments. In \ref{sec:rel-studypart} we name controlled studies, which were conducted with individual participants who consumed some kind of synthetic content. Our own paper should fall under the latter category as well. And in \ref{sec:rel-work-counteringdf} we added some works about countering maleficent synthetic media.

\section{Historical and socio-geographical Context}
\label{sec:hist-context}
As stated in the introduction, synthetic media is a rather new trend, mainly driven by the advance of \gls{genai}. It can be dated back to the earliest of 2017 when deepfake face swaps surfaced to the broader public. It is remarkable that the term \textit{fake news} is also a very young topic on its own and developed around the same time as deepfake face swaps around 2016. \\
For completeness, it should be noted that the \citet{merriam-websterdictionaryRealStoryFake} dates the first use of fake news back to the year 1890. Surely skepticism has always existed towards politics and media, but it surely wasn't as big of a topic until Donald Trump entered his presidential campaign in 2016 as can be seen in the Google trend analysis in Figure \ref{fig:gtrend-fake-news} and \ref{fig:trust-us}. \\
That said, it wouldn't make much sense to look for related work earlier than 2016/2017. Therefore, we focused our attention to the timeframe where deepfakes came into existence and fake news became a publicly relevant topic.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/gtrends_fakenews_1011-2311.png}
  \caption{Google Trends of the Term "Fake News" 2004-2023}
  \label{fig:gtrend-fake-news}
\end{figure}

In our study, we investigate a German-speaking population. Therefore, we want to add that most papers that are focused on a certain territory (mostly US in relation of Trump) can only be included with the caveat that they might not directly apply to the German cultural space as well. To illustrate one difference, we can refer to Figure \ref{fig:trust-ger} in comparison to Figure \ref{fig:trust-us}. \\
While trust among the American public decreased around 2016, no impact can be seen in Germany at this time. It took until the years of the COVID-19 pandemic to experience a decline in Germany as well. It remains to be seen how this trend will continue in the following years.

\begin{figure}[h]
  \centering
  \centering
  \includegraphics[width=0.75\linewidth]{./graphics/trust-america mainstream.png}
  \caption{Trust in American Mainstream Media 1996-2017 \cite{allcottSocialMediaFake2017}}
  \label{fig:trust-us}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{./graphics/FGW-Trust-in-ARDZDF.png}
  \caption{Question: How high is your trust in \gls{psm} ARD \& ZDF? (2015-2023) \cite{zdf-politbarometerVertrauenGlaubwuerdigkeitBerichterstattung2023}}
  \label{fig:trust-ger}
\end{figure}

Surely it is impossible to pinpoint one singular event, person, or technology as the starting point for trust issues within a society. We chose to present this research from a technological perspective. And while we define deep fakesas the umbrella term for all synthetically generated Fake News, we will chronologically encounter face swaps first, as they are the oldest (2017) of the relevant technologies. The next deepfake tool to come around was probably Wav2Lip in 2020. Please bear in mind that literature published before the \gls{genai} boom of 2022 refers to deep fakesas face swaps in most cases, simply because other technologies were non-existent or irrelevant to the public until very recently. The term deepfake was often used analogously with face swaps until other technologies advanced, such as synthetic voices, lip-remapping, and today's image generators.

\section{Perceptions and Implications through Secondary Indicators} 
\label{sec:rel-secondary}

Investigating deepfake technology through secondary indicators, such as video comments or blog posts, is essential to comprehensively grasping public sentiment and articulating feelings surrounding this technological advancement. Analyzing these sources provides valuable insights into how individuals perceive and express their concerns, contributing to a nuanced understanding of the societal implications of deepfake technology. By looking into the diverse array of opinions presented in video comments and blog posts, we gain a more holistic view of the multifaceted landscape surrounding deepfakes.

\subsubsection*{News Article Reviews}
\citet{westerlundEmergenceDeepfakeTechnology2019a} provides a literature review on "emerging scholary literature" in addition to publicly available news articles. They used this data to reflect on the benefits, threats, and examples of current deepfakes and how to combat them. The authors list several benefits in the regions of media production, educational media and digital communications, games and entertainment, social media and healthcare, material science, and various business fields, such as fashion and e-commerce. As examples, they name several entertaining deepfakes of famous actors or a museum installation that brought Salvador Dalì back to life. \Citeauthor{westerlundEmergenceDeepfakeTechnology2019a} also concludes: "According to our study, deepfakes are a major threat to society, the political system and businesses[...]". \\
They reached this conclusion based on the existence of countless pornographic deepfakes, and multiple political fakes. The Obama/Peele Fake is quoted again, but also a viral video of the American politician Nancy Pelosi and another fake of Donald Trump. The authors evaluated these videos as having limited political influence, but they also provide two examples where they had consequences. \\
In one case from 2018, a deepfake of Gabon's long-unseen president, Ali Bongo, who was believed to be in poor health or dead, was cited as the trigger for an unsuccessful coup by the Gabonese military. In the other case, a viral clip from Malaysia showed a man's confession to having sex with a local cabinet minister and caused political controversy.

About a possible solution \Citeauthor{westerlundEmergenceDeepfakeTechnology2019a} names four fields: "1) legislation and regulation, 2) corporate policies and voluntary action, 3) education and training, and 4) anti-Deepfake technology". Section \ref{sec:rel-work-counteringdf} will give further insight in some countermeasures.

\subsubsection*{YouTube Comment Analysis}
The works of \citeauthor{leeBelieveNotBelieve2021} took a different approach and moved closer to the corupus delicti. They picked the top 10 deepfake videos on Youtube and provided a framing analysis of the audience's comments. The word \textit{framing} is important here and requires an explanation. Framing refers to how the certain piece of information is contextualized. Or, in the words of the authors, "[...]The \textit{valence framing} effect explores how information, framed positively or negatively, may systematically affect audience responses". From the 10 most-viewed deepfake YouTube videos, they gathered 88.362 comments, from which they chose 2.689 (3\%) randomly sampled comments for further analysis. They then analyzed how the videos were framed and looked at the framing of the comments.\\
Five videos were positively framed, three were classified as neutral, and two had negative framing. Regarding the comments, the authors found that there were quite distinct groups who recognized positive or negative potential for deepfakes but rarely see both a positive and negative side to the technology. These distinct groups were also in line with the framing of the corresponding videos. \\
The authors conclude that the audience is largely influenced by the framing of the video and remark that this situation might not be ideal for the education of all viewers as their opposite opinion is always missing. This is quite an easy argument to follow. Of course, more shades of grey are always better for a balanced discours. However, we have to add doubts about the depth of information one can extract from these comments. To make an assumption, a video comment will rarely be a detailed, dialectic thought but more of a spontaneous reaction to the seen video. That said, it seems hard to make a consise assessment of the viewers' thoughts. \\
Nonetheless, the findings remain interesting to remind us that a balanced coverage of such a delicate topic is important.

\subsubsection*{Reddit sentiment Analysis}
Similar to the works of \citeauthor{leeBelieveNotBelieve2021}, \citeauthor{brooksPopularDiscourseDeepfakes2021} qualitatively investigated the framing of deepfakes but instead used posts from the platform Reddit. In contrast to the previously mentioned analysis of YouTube videos, Brooks' work is not as detailed in regards to specific numbers. \\
It is still quite interesting because \citeauthor{brooksPopularDiscourseDeepfakes2021} chose posts from December 2017 until February 2018, which is exactly the timeframe and platform where and when deepfake pornography emerged. During analysis, two primary themes emerged: (1) risk to individuals by way of personal abuse and reputation decimation, and (2) risk to society in terms of war threat and societal impact. As an interesting sidenote, the authors mention parallels to the rise of the internet, where similar threats were assumed. To conclude, the author points to the need for various solutions for all the different problems that arise from the emergence of deepfakes.

According to \cite{brooksPopularDiscourseDeepfakes2021}, personal responsibility and individual literacies are often formulated as a solutions but do not take into account differences in culture, language, or digital literacies. The author calls for sophisticated media forensics in order to detect fake material. A wish that which might not be sustainable in the long run, as will be discussed in Section \ref{sec:rel-work-counteringdf}. To give one example, Brooks names blinking eyes as a fake indicator. But after an imperfection has been caught by an automatic forensic system, it can be used as input to further improve the fake. In the end, Brooks also states the need for increased responsibility on platforms to reduce the spread of deepfakes.

% Reviews with studies on actual people
\section{Controlled Studies with Participants}
\label{sec:rel-studypart}

While we've explored the broader picture using secondary indicators and public discourse, it's crucial to zoom in and examine how individuals directly respond to deepfake technology. This section shifts focus to measurements with study participants, aiming to uncover detailed insights into how deepfakes impact individuals' trust, perceptions, and behaviors and make them tangible with studies.

\subsubsection*{Vulnerability to microtargeting}
Whether people indeed “fall for” deepfakes is unclear and understudied, but not unimaginable \cite{dobberMicrotargetedDeepfakesHave2021}. \citet{dobberMicrotargetedDeepfakesHave2021} particularly investigated what effect deepfakes can have when amplified through microtargeting in a fictional scenario. In a study, they interrogated 278 participants to answer the research question of how the attitudes of supporters of a depicted politician's party are affected by a deepfake video, meant to discredit the political candidate. As a blocking variable, they chose Christian religious identity. By answering questions about religion, the participants were placed into Christian and non-Christian groups and then randomly served the original video, or the deep-faked video of a Dutch-Christian-Democrat politician. In the faked version, the politician jokes about Christ's crucifixion. The authors suspected that this scenario "would move attitudes because the politician is a prominent Christian politician and the base of his party is to a large extent Christian". \\
The authors found evidence that the attitude towards the politician dropped significantly in the experimental groups.

Statistical evidence was also found for the hypothesis that the microtargeted Christian group was more effected by the statements than the non-Christian group. All the results indicate that it is indeed possible to stage a political scandal with a Deepfake. Intestingly their findings differed from earlier authors, who found no effects of disinformation on political behavior and attitudes. They state: "[...] indeed deepfakes are a more powerful mode of disinformation in comparison with the false news stories studied by Guess et al. (2018) and the Russian Twitter trolls studied by Bail et al. (2020)" \cite{dobberMicrotargetedDeepfakesHave2021}.

\subsubsection*{Sowing Uncertainty}
The deepfake face swap example of Obama, voiced by Jordan Peele, has been mentioned several times. \citeauthor{vaccariDeepfakesDisinformationExploring2020} conducted a study with exactly this material. The authors tested how a sample of 2.005 participants responded to three variants of the Obama/Peele fake. They have split up the video into three separate videos. Two of them were considered deceptive because they didn't include the revelation at the end, where the deepfake was explained. Of the two deceptive videos, the first one was four seconds long and included Obama "saying" that Donald Trump is a complete dipshit. The second video was 26 seconds long and included more text. The third video contained the explanatory part of the original video, where the fake is explained and played side by side with the actor voicing Obama's appearance.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{./graphics/obamafake.png}
  \caption{Assessment of the truthfulness of the Video, grouped by Treatment. \cite{vaccariDeepfakesDisinformationExploring2020}}
  \label{fig:obamafake-eval}
\end{figure}

The following results can be seen in Figure \ref{fig:obamafake-eval}. The authors found that "overall, only 50.8\% of subjects were not deceived by the Deepfake. This finding is surprising given the statement was highly improbable" \cite{vaccariDeepfakesDisinformationExploring2020}. 
It is also visible that the deception rate was quite consistent over all three videos. Much more interesting was the author's second finding: "Importantly, however, the results support H2 — watching a deepfake that contains a false statement that is not revealed as false is more likely to cause uncertainty" \cite{vaccariDeepfakesDisinformationExploring2020}. \\
In an evaluation of the Obama/Peele fake, \citet{vaccariDeepfakesDisinformationExploring2020} conclude: "We have shown that political deepfakes may not necessarily deceive individuals, but they may sow uncertainty which may, in turn, reduce trust in news on social media." The authors also add that these detrimental tendencies can lead to a downward spiral of trust loss in media and news in general. \\
In this context, it is important to add that these cases might not even need manipulated video to spread fake news and uncertainty. Text can also suffice to spread misinformation, as could be seen during the COVID-19 pandemic and the often-mentioned social media channels \cite{naeemExplorationHowFake2021}. Another awful example of how powerful just text alone can be is the Q-Anon movement. \\
\citet{zeeuwTracingNormieficationCrossplatform2020} investigated "[...] how ideas and objects travel from fringe online subcultures to large audiences on mainstream platforms and news outlets". The Q-Anon subculture has led to several deaths in the real world (Pizzagate) and was related to the January 6 United States Capitol attack.

\subsubsection*{Uncanny Valley Effects}
\Citeauthor{weismanFaceUncannyEffects2021} studied the effects of 3D animated talking head avatars, specifically doppelganger recreations of the participants. These 3D characters could then be used to convey messages to their real-life counterparts. The messages were one pro- and one anti-AI statement recorded from the corresponding participant in advance. Using the participants' own voices, the avatar was both voiced and animated. The animation was accomplished using the software Reallusion CrazyTalk8. Two example avatars are depicted in Figure \ref{fig:uncanny-avatars}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{./graphics/uncanny-avatars.png}
  \caption{Doppelganger Avatars created with CrazyTalk8 \cite{weismanFaceUncannyEffects2021}}
  \label{fig:uncanny-avatars}
\end{figure}

CrazyTalk8 uses an algorithmic approach to movinf the target's head and lips based on audio; however it is not disclosed how the process exactly works. It is imaginable that the discrepancy between the participant's natural and real voice and the rather imperfectly looking and animated 3D character induces a significant amount of uncanny valley discomfort. This is amplified as the doppelgangers portray the participants themselves, an appearance that is very familiar. The authors willingly produced a highly uncanny perception in order to measure the trust loss due to the uncanny appearance. In the study, some participants were exposed to their own voice and avatar (doppelganger), as well as another avatar and voice that was unfamiliar to them. That way, they could dial the uncanny valley feeling up and down. It was higher when the participant was subjected to its own doppelganger and lower when it was a different person.\\
The authors concluded that the uncanny valley effect mediated participant-affect-based AI trust. Uncanny valley perceptions were negatively related to affect-based trust. "Presenting individuals with a talking head featuring their own face decreased affect-based trust toward AIs relative to talking heads featuring a stranger's face or relative to simple audio playback" \cite{weismanFaceUncannyEffects2021}. \\
We've cited this paper because we also introduced an uncanny vally test case as a negative control group into our study (see Section \ref{sec:study design}). 

Different from our approach, we want to remark that the study by \Citeauthor{weismanFaceUncannyEffects2021} is not suited to measure trust issues caused by artificial intelligence technology accurately, as there is no real control group that uses real footage or better AI technology (without or with less uncanny valley). We can infer that the reduced trust is caused predominantly by the uncanny valley effect and not by the fact that it was created by AI-aided algorithms. We tried to address this study design difficulty by including real, non-fake material to be tested against in order to receive a base-line measurement of trust for real footage, free of any \gls{genai}.

\subsubsection*{Trust in synthetic Voices}
So far, we have mostly mentioned papers that studied the visual part of deepfakes. As explained earlier, this is most likely linked to the unavailablity of easy-to-use, high-quality audio voice clones. In recent times, this has changed. Several easy-to-use \gls{tts} services can be found on the internet. \Citeauthor{heiselbergAutomatedNewsReading2022} provide recent research about synthetic voices, or what they call it "neural voices". \\
They provide a concise summary of their findings: "Results show that the participants divide into two types: the perspicacious listeners who realize or suspect that the news reading is artificially synthesized and, to some degree, are annoyed by it, and the oblivious listeners who believe the news is read by a human and are predominantly positive towards it. Participants from both groups pay particular attention to voice emotionality when evaluating the appropriateness of the neural news reader. Also, they tend to attribute human characteristics to the neural news reader. The participants single out the news messages as well as the media organization behind the news broadcast, rather than the neural voice itself as critical components constituting credibility. Transparency is of great importance when applying a neural voice in a news broadcast, since it is a prerequisite for credibility" \cite{heiselbergAutomatedNewsReading2022}.

For our work, this is especially relevant as it is set up in the environment of a news broadcast as well. In contrast to our study, it focused on radio and the voice component alone. This reduced the surface are where the uncanny valley could come into play. In contrast to our work, the authors conducted qualitative interviews instead of a quantitative analysis. To somewhat include a qualitative dimension as well, we gave our participants the option to justify their answers in free text forms. 

\subsubsection*{The Disclosure of AI reduces Trust}
As probably the most recent paper, we include \citetitle{toffTheyCouldJust2023} by \citeauthor{toffTheyCouldJust2023}. At the time of writing (December 2023) the cited paper has just been released as a pre-print and has not yet been peer-reviewed. \\
The authors conducted a study using actual AI-generated journalistic content. Their focus lied on the effect of labelling AI-generated content. The studied sample consisted of audiences in the US and differnciated between polarized partisan lines. \\
They found that, on average, audiences perceived news labeled as AI-generated as less trustworthy, not more (see Figure \ref{fig:toff-trust}). 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{./graphics/toff/Trust in news.png}
  \caption{Gaps in perceived trustworthiness associated with disclosure about the use of generative AI varied as a function of prior levels of trust in news, holding all other variables at their mean values \cite{toffTheyCouldJust2023}.}
  \label{fig:toff-trust}
\end{figure}

Furthermore, they found that these effects were largely concentrated among those whose pre-existing levels of trust in news were higher to begin with, and among those who exhibit higher levels of knowledge about journalism (refered to as PNK) as depicted in Figure \ref{fig:toff-PNK}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{./graphics/toff/PNK.png}
  \caption{Gaps in perceived trustworthiness associated with disclosure about the use of generative AI varied as a function of procedural news knowledge (PNK), holding all other variables at their mean values. \cite{toffTheyCouldJust2023}.}
  \label{fig:toff-PNK}
\end{figure}

They also found that negative effects associated with perceived trustworthiness are largely counteracted when articles disclose the list of sources used to generate the content. \\
In contrast to our work, \citeauthor{toffTheyCouldJust2023} used AI-generated text as the object of investigation. This is a point where our works differ, because ours had humans "in the loop" at several touchpoints. We used AI to aid the production side of how the content was conveyed, not the content itself.

After having analised our study we can neither confirm nor reject the results of \citet{toffTheyCouldJust2023}. Our study design simply induced too many disturbances to the measurements of the disclosure effect. The authors' focus on text only was very well suited to measure this in an isolated study, contrary to what we did. 

\section{Countermeasures}
\label{sec:rel-work-counteringdf}
\subsubsection*{Technical Solutions are not sufficient}
As soon as we encounter negative use cases of synthetic media, the call for efficient countermeasures arises. Since we are dealing with deep learning techniques, the a frequent suggestion is to counter "fire with fire" and use deep learning to train a synthetic media detector. At this point, it must be mentioned that anti-deepfake technology, like forensics and AI-based detection, will always have its boundaries. The efficacy will always depend on the technology's ability to detect deepfakes. With better detection come better methods for generation. It's an arms race that improves both parties and initially gave \gls{gan}s their name.

Because of that, other solutions in the domain of \textbf{media provenance} could provide more safety than forensic detection. In this context, "Project Origin" has to be mentioned. An alliance of media producers and broadcasters, who want to develop a "technical provenance approach, in conjunction with media education and synthetic media detection techniques [and] help to establish a foundation for trust in media" \cite{ProjectOrigin}. The alliance includes organisations such as the \textit{BBC, Radio Canada, New York Times and Microsoft}. This led to the creation of the \gls{c2pa}, the formulation of a technical standard to achieve the goals outlined earlier. This project is today backed by major hardware (\textit{Canon, Nikon, Sony, Leica, Panasonic, ARM}) and software companies (\textit{Adobe, Microsoft, AWS, AVID}), media resellers, distributors, and providers (\textit{BBC, Radio Canada, dpa, france-tv, shutterstock, universal music}. \gls{c2pa} is currently being implemented by the industry.

All in all, technical solutions can only contribute a minor part to solving the problem of fake news with synthetic media. The following authors have conducted research in these areas and provide alternatives to just technical solutions. 

\subsubsection*{The positive Effect of Media Literacy}
\citeauthor{hwangEffectsDisinformationUsing2021} investigated the protective effect of media literacy education in the context of deepfakes. We have to point out that the authors use the term deepfake frequently in their paper, but it seems that their understanding of "deepfakes" is somewhat analogous to "fake news". After all, they do not mention deepfakes in the context of Artificial Intelligence once, which contributes to our understanding of their interpretation. \\
Their study also does not use any sort of AI-generated content but rather exposes the participants to fake news in textual form. Prior to the test, participants received two types of education: general education about disinformation and deepfake-specific education. A control group, which received no education, was also included. \\
We included this study as the authors were able to show that an increase in media literacy of any form has a protective effect against fake news.

While our study did not explicitly collect any data about media literacy and use it as a dependent variable, we tried to infer such preconditions via age or occupation. Unfortunately we couldn't find any indications of an influencial effect. This was most likely due to other stronger factors, underlining the need for an isolated study about this topic.

\subsubsection*{The positive Effect of Priming}
Actually related to deepfake face swaps is a study by \citeauthor{iacobucciDeepfakesUnmaskedEffects2021}, where they tried to measure the effects of information \textit{priming} and \textit{bullshit receptivity} on deepfake recognition and resulting sharing intention. Information priming in this context is defined as "an improvement in performance in a perceptual or cognitive task, relative to an appropriate baseline, produced by context or prior experience" \cite{iacobucciDeepfakesUnmaskedEffects2021}. The priming serves the purpose of increasing awareness about certain topics and therefore can be understood in a similar manner as the media literacy suggestions of \citeauthor{hwangEffectsDisinformationUsing2021}. \\
The amount of priming is measured against the participants ability to recognize deepfakes (DF recognition). This was measured by asking: "The similarity of the remake to the original scene is due to the actor's abilities and not to digital video editing technologies." In addition to the DF recognition, the authors also determined a bullshit (BS) receptivity index for each participant. This index is being measured according to \cite{pennycookReceptionDetectionPseudoprofound2015}: Participants were asked to rate the profoundness of pseudoprofound sentences on a 5-point scale from 1 = "not at all profound" to 5 = "very profound" (Examples: "Your teacher can open the door, but you must enter by yourself"; "A river cuts through a rock not because of its power but because of its persistence"). Higher scores hint towards an inability to detect pseudoprofound statements. \\
Based on participants BS receptivity and DF recogntition, the authors surveyed the measurements before and after priming. \\
\citeauthor{iacobucciDeepfakesUnmaskedEffects2021} summarize their results in three ways: \\
Participants in the priming condition showed greater DF recognition than the participants in the control condition. \\
Priming users with DF knowledge influences their ability to recognize a DF, but only when they are not strongly inclined to the reception of BS. \\
Their hypothesis about reduced sharing intention could also be confirmed as higher levels of DF recognition (induced through priming) reduce intention to share the video through attitudes toward the video. \\
Its total "results indicate that the development of strategies to counter the deceitfulness of DFs from an educational and cultural perspective might work well, but only for people with a lower susceptibility to believe willfully misleading claims. Finally, through a serial mediation analysis, we show that DF recognition does, in turn, negatively impact users' sharing intentions, thus limiting the potential harm of DFs at the very root of one of their strengths: virality. We discuss the implications of our finding that society's defense against DFs could benefit from a simple, reasoned digital literacy intervention" \cite{iacobucciDeepfakesUnmaskedEffects2021}.

The listed methods all provide some degree of mitigation to the deepfake problem on an individual level. At the size of a society, laws and policies should help as well. Unlike public opinion, there are already laws in place that should provide societal protection for both personal harm and disinformation tactics. Defamation, infringements of one's own image, privacy intrusion — these are all offenses not exclusive to synthetic media misuse. Therefore, no specific AI Act is needed to counter misuse. Because of that, we did not research lawmaking literature in this context, as it is already extensively studied in other domains. The missing component to counter the threats might be the consequent enforcement of the existing laws in the virtual space. 

\subsubsection*{Literature Conclusion}
We have listed several studies about the effects of some forms of synthetic media. In regard to mitigation at the individual level, education, increasing media and deepfake literacy are efficient ways to reduce threats from deepfakes. Overall, we had to realize that the available literature rarely covers newer forms of \gls{genai}. \\
We conclude that research involving \gls{genai}-created media is still scarce. This is logical due to the novelty of the technologies, their complexity, and the availability of tools for creation. \\
Generative AI is advancing at such a rate that it's impossible to keep all developments in sight. Research has just caught up with the outcome of face swap deepfakes, which today look antiquated in the light of ChatGPT and \gls{sd}. \\
It is obvious that the research community has to continue to follow the call from \citet{hancockSocialImpactDeepfakes2021} and study. A call that the authors of this paper were happy to comply with.

\chapter{Implemented Technologies}
\label{chap:implementation}
After we have established a theoretical background as well as an overview of existing works, we will now move on to the means we used to conduct our studies. We will start with the description of the technical implementation and then move on to the actual study. Still, in order to better understand the reasons why we chose certain technologies, we want to first describe one central thought in our work: the spectrum of artificiality depicted in Figure \ref{fig:spectrum}.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/spectrum-art.png}
  \caption{Spectrum of Artificiality}
  \label{fig:spectrum}
\end{figure}

On the left end, we have 100\% unaltered footage, whereas on the right end, we have a very uncanny-looking and sounding, synthetic case. In between these extremes, the amount of artificiality increases from left to right. The spectrum set our goals for what we wanted to create and led to the selection of tools. A more detailed description of the thought process behind the study design will be provided in Section \ref{sec:study design}.

Most of the development and training has been conducted on a remote Linux server of the \gls{hff} and film munich via SSH. Many projects featured a Gradio web UI. That way, these tools could easily be accessed from remote hosts via port-forwarding. The \gls{hff} workstation featured an Intel(R) Xeon(R) Gold 6254, 128GB of memory and a Nvidia RTX A6000 GPU with 48GB of VRAM. \\
The code was checked in at a repository via the \gls{br} at gitlab.ard.de. CI/CD was provided with a privately hosted gitlab runner. \\
During the works, two projects were created. The first, \gls{dvt}, provided all necessary implementations for the synthetic voices, including dataset pre-processing and training. The second is an implementation of \gls{w2l} with face upsampling using GFPGAN. Both of the projects were built in a containerized way using Docker to ease the installation of dependencies while working on the remote machine without root access. This also ensured high portability across systems in case we had to switch to another machine. Docker images were built with a CI pipeline and then pushed to a private container registry. \\
An instance of \gls{sd} was also used without containerisation.  \\
For one of the test cases (Actor MoCap in Figure \ref{fig:spectrum}), unreal engine was used on a Windows workstation at the \gls{br}.

In the following, we will give a closer insight into the generation process of several assets, such as \gls{tts}, \gls{rvc}, \gls{sd} and unreal engine.

\section{Voice Cloning Toolkit}
\label{sec:dvt}
While we had some previous experience in the video domain (Face-swapping and Unreal Engine), we had almost none in the audio domain. Therefore, we had to figure out feasible ways to create synthetic voices. There are some commercial solutions available that deliver excellent results, but we wanted to see how far we can get with open-source solutions. In the domain of face-swapping there are already integrated toolkits for data gathering, quality control, pre-processing and training (\gls{dfl}). This is not the case for voice cloning, which is why we decided to combine several tools into one. These were CoquiTTS, an open source \gls{tts} engine and \gls{rvc}, which is used for \gls{v2v} generation (see Sections \ref{sec:tts} and \ref{sec:v2v}). 

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/dvt-screen.png}
  \caption{Start screen of "Danilo's Voice Toolkit" DVT}
  \label{fig:dvt-interface}
\end{figure}

\gls{dvt} is a command-line interface (Figure \ref{fig:dvt-interface}) and serves as a wrapper for CoquiTTS and \gls{rvc}, and adds methods for importing raw training data and preprocessing it. \\
\gls{dvt} is launched as a Docker container with several forwarded ports. The current directory at launch time is considered to be the working directory for a certain project and is set as the workspace for the project. Several folders are created for storing the data, downloading the required models, and persisting them on the local filesystem. After the setup is done, one can start working by placing files in the corresponding \verb|AUDIO INPUT| folder. For the case \gls{dvt} is running on a remote machine, as it was in most of our cases, we have added a web-based filebrowser to the Docker container, which needs to be started by calling \verb|filebrowser| in \gls{dvt}. Once it is up, we can browse to its port, explore the filesystem and upload the required data. This is only reasonable for smaller datasets. In the case of our TTS Training Data (30 Gigabytes), downloading a tar archive via \verb|wget| and unpacking it was more reliable. 

In the following, we will treat the production and training processes of the mentioned technologies (\gls{tts} and \gls{rvc}) in general, then explain the corresponding training data requirements, and in the end, summarize everything by walking through the whole workflow, from data gathering until training the model. Inference is almost trivial and is well documented for both \gls{tts} and \gls{rvc}.

\subsection{Text-to-Speech}
% General
CoquiTTS is a comprehensive \gls*{tts} tool that can be used as open-source, that can be deployed on own systems and at the same time drives a commercial variant called "Coqui Studio". As explained in background Section \ref{sec:tts} there are various ways to realise text to speech. We decided to go with the VITS model. Coqui uses so-called "receipes" to set up a training run. As they are language-specific, we relied heavily on the documentation of \Citeauthor{mullerThorstenVoice2023} who contributed a lot of work to advance the German language within the Coqui project. \\
With the recipes by \Citeauthor{mullerThorstenVoice2023} it is quite easy to get started. It goes without saying that each training has the option of extensive hyperparameter tuning, but this was way beyond the scope of this paper, and we therefore went with the basic settings provided by \Citeauthor{mullerThorstenVoice2023}, except that we increased the batch size to speed up training on our A6000. \\
Most of our development effort did not go into the training but into the dataset preparation, which we will disclose in the next paragraph.

% Preprocessing need
\textbf{Data Requirements} \\
Regarding the dataset structure, it seemed simple: the central resources for training a VITS TTS model are voice samples and corresponding transcription. We discovered from the Coqui community that around two to four hours of speech should deliver the best quality. In order to avoid \gls{oom} errors, the audio needs to be split up into segments of a maximum length of 10 seconds. Lastly, each and every clip needs accompanying text (example in Figure \ref{fig:metadata.csv}).

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/tts/csv.png}
  \caption{Example of the TTS training metadata.csv file}
  \label{fig:metadata.csv}
\end{figure}

But getting to the formatted Dataset turned out to be quite time-consuming with some space for improvements using automation.

% Concrete Workflow
\textbf{Concrete Workflow} \\
We want to give a detailed example of how we worked on our \gls{tts} dataset. The final workflow is outlined in Figure \ref{fig:dvt-tts-wf}.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/tts/tts prpro.png}
  \caption{Data preprocessing Workflow for TTS training: using \gls{prpro} and Audacity for Workaround}
  \label{fig:dvt-tts-wf}
\end{figure}

A good voice clone starts with a smart target selection. Therefore, we had to determine a suitable environment for our test cases. As we have hinted earlier and will elaborate the study design further in Section \ref{chap:study}. TTS voices, in general, perform best with consistent voices where no special (emotional) intonation is required. A newscast is perfect in this regard because of the neutral narration of the content. \\
According to this requirement, we aimed for four hours of speech extracted from the \gls{br} news show "BR24". We ended up with 60 shows, which acounts for a total runtime of 30 hours. From each, we extracted approximately 5 minutes of the anchorman's voice.\\
In the end, our dataset contained 2002 individual clips with transcription. Getting there from 60 hours of news broadcast was quite an undertaking that could never be feasible without automation, which is where our partially automated preprocessing workflow comes in. \\
We used several tools to aid the dataset creation process, based on various open-source projects \cite{micaAudioSplitterUsing2023}, \cite{harperEndtoEndToolkitVoice2023}: \textit{Pyannote speaker diarization} helps to distinguish multiple speakers within one audio track. \textit{OpenAI Whisper} provides audio-to-text transcription. \textit{NVIDIA NeMo} is used for the normalisation of the transcribed texts. To give an example for normalisation, numbers like "42" would get normalised to the corresponding word "forty-two". This is necessary for the training process to avoid ambiguity, as all input texts are being normalized before being handed over to the \gls{tts} engine. \\
First, we tried a fully automated way for the dataset creation, which is depicted in Figure \ref{fig:dvt-tts-original}.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/tts/tts dvt only.png}
  \caption{Fully automated Data preprocessing Workflow for TTS in \gls{dvt} (not practicable)}
  \label{fig:dvt-tts-original}
\end{figure}

In this case, we input one or multiple audio files and start the process. Based on the tools we've just mentioned, the audio gets transcribed and marked with a speaker ID.
It is then cut up into pieces of a maximum length of 10 seconds, based on the transcription timecodes, and placed in a dedicated directory based on the speaker ID. The folder with the most files, which is the folder of the anchorman's voice, is moved to the TTS Dataset directory, where it is again transcribed and receives the corresponding metadata.csv file with the necessary transcription for \gls{tts} training (see Figure \ref{fig:metadata.csv}).

This process worked quite well, besides rarely adding false speakers to the anchorman's directory. But there is one downside to this method, which, unfortunately, makes it unusable. The transcription timecodes provided by Whisper, which are used to cut the audio into pieces, are not accurate enough and often cut parts of a spoken word away. That results in the corresponding TTS voice sounding chopped up as well. This issue could be fixed by using a cutting mechanism which that relies on silence in the audio as a cutting signal instead of the transcription timecodes. Although an implementation could have been derived from one of the referenced github projects, we opted against it because we would then lose the speaker diarization funcionality, which we needed, because our news segments contained so many different speakers. \\
Instead, we opted for a more time-consuming semi-automatic workflow (Figure \ref{fig:dvt-tts-wf}) in conjunction with the previously mentioned tools: Instead of cutting the the audios in \gls{dvt}, we added subtitle export functionality (\verb|extractSpeakerMarker|)for each speaker as a csv file. We then import the subtitle files of the anchorman into the video editing software \textit{\gls{prpro}}. Unfortunately, this is not natively supported by \gls{prpro} which is why we had to rely on an extension called \textit{Markerbox} \cite{montgomeryMARKERBOXFreeMarker}. This turned out to be the quickest alternative to reverse engineering the \gls{prpro} XML format or purchasing software. In the end, we ended up with markings in the timeline (see Figure \ref{fig:premier-markers}). 

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/tts/premier with markers.png}
  \caption{Audio File in \gls{prpro} with Markers of main Speaker}
  \label{fig:premier-markers}
\end{figure}

Now we were able to manually cut away all sections of other speakers very quickly as we had the visual aid of the markers in the timeline. Once we had isolated the main speaker, we exported the audio clip. From each 30-minute news segment, we extract approximately five minutes of anchorman-only talk.\\
Next, we needed to chop up the five-minute pieces into audios of  maximum length of 10 seconds based on silence detection. Though this could have been automated, as discussed earlier, we didn't want to spend more time on such implementations but just get the dataset ready as quickly as possible. Instead, we chose \textit{Audacity} to confidently detect silence and render out all clips at once. \\
Now we had all clips cut out cleanly, so we uploaded them again into \gls{dvt} to finalize the dataset creation by performing one last transcription with text normalization (\verb|transcribe_audio|) to end up with a correct metadata.csv file (see Figure \ref{fig:metadata.csv}). \\
Now, given that all files have been generated and placed in the right directories, starting a \gls{tts} training is rather easy. Within \gls{dvt}, we invoke \verb|tts_train| and we will be guided to start a new training run with a given name and some minor parameters,like batch size and training epochs. While the training will starts, a tensorboard is also spun up in the background in order to monitor the training process. It can be accessed with a web browser if ssh port forwarding is setup properly (see Figure \ref{fig:tensorboard}).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{./graphics/tts/tensorboard.png}
  \caption{CoquiTTS Tensorboard \cite{TensorboardPngMbarnig2022}}
  \label{fig:tensorboard}
\end{figure}

The tensorboard provides many interesting insights into the training process, plotting loss graphs, showing mel spectrograms, and preparing generated audio files for easy playback. By judging these different sources, one can determine easier, whether the training is finished or not. 

% Inference
\textbf{Inference} \\
Now that the model is trained, inference is straight-forward. For further details, we encourage you to check out the CoquTTS documentation. A basic CLI command to synthesize a sample looks like this:
\begin{lstlisting}
  tts --model_path "path/to/checkpoint.pth" --config_path "path/to/config.json" --out_path "path/to/outputFile.wav" --text "Text to be synthesized."
\end{lstlisting}

\textbf{Possibilities for quality improvements} \\
After the \gls{tts} audio is generated, it might be necessary to do some manual corrections to the audio track. Most imperfections are related to odd sentence flow or chopped consonants towards the end of a sentence. This can be done with any editing software. We rarely had to correct anything for our model, but certainly tried to edit the audio to the right positions of the video so that it matched the scene as good as possible. \\
In addition, we could also upscale the quality by applying \gls{rvc} on top of the \gls{tts}-generated audio. While \gls{rvc} could not correct odd sentence flow or chopped words it could add some depth to the speech quality. We decided against this improvement to have more distinctiveness between the test cases, but in a production system, this might be a viable solution in order to improve \gls{tts} quality. 

\subsection{Voice-to-Voice Conversion}
%General
To convert our speech recordings to the voice of the anchorman, we have used \gls{rvc}. Compared to \gls{tts}, \gls{rvc} is straight-forward. Regarding \gls*{dvt}, almost no adaptations to the original \gls*{rvc} \cite*{RVCProjectRetrievalbasedVoiceConversionWebUI2023} code have been made. \gls*{dvt} mostly serves as a CUDA runtime environment to provide all drivers, dependencies, and AI models required to run \gls{rvc}. We've added the aforementioned \verb|filebrowser| to make file transfers to a remote GPU server easier. Of course, we can employ our preprocessing tools to aid the dataset generation process.\\
As depicted in Figure \ref{fig:rvc-gradio}, \gls{rvc} features a Gradio user interface with several tabs at the top. Most importantly, training and model inference. For training, most hyperparameters are already set; only the dataset location needs to be set. Inference is also done within the GUI. \\
\gls{rvc} was initially based on a project for singing voice conversion, a feature that is implemented in \gls{rvc} as well. During inference, voices, as well as singing voices, can be converted to previously trained models. Before converting a singing voice we would need a clean vocal track. For that reason, the developers added the \gls{uvr} to a separate tab of the GUI. With the utility, the main vocals can be separated from background music in order to convert the voice properly without background noise.

%Data requirements
\textbf{Data Requirements} \\
For a good \gls{rvc} model, only 10 minutes of speech are required. It is also less important than for \gls{tts} that sentences are complete. If a word is cut off, we haven't experienced any issues. Because of that, we can fully use the speaker diarization and cutting process we initially built for \gls{tts}. 

%concrete Workflow
\textbf{Concrete Workflow} \\
As can be seen in Figure \ref{fig:rvc-wf}, the preprocessing workflow is much leaner compared to \gls{tts}.
\begin{figure}[h]

  \centering
  \includegraphics[width=1\textwidth]{./graphics/rvc/rvc-workflow.png}
  \caption{Data preprocessing Workflow for RVC Training}
  \label{fig:rvc-wf}
\end{figure}

As we know that we can extract approximately 5 minutes from every 30 minutes of news shows; we need two of them for the 10 minutes of training material. They get placed in the INPUT directory. Invoking DVT's \verb|split_audios| method triggers the speaker diarization and segment splitting workflow based on a transcription and corresponding timecodes. \\
We end up with a subdirectory for each speaker of each input audio. We could now use \verb|filebrowser| to listen through the speakers, and then move the right audios into the \verb|/DATASET/RVC| directory. Or we can use \verb|move_most| which will go through all speaker folders and move those with the most data into \verb|/DATASET/RAW/SELECTION_HERE|. Now we can move all data from here into \verb|/DATASET/RVC|. \\
Once 10 minutes of snippets. all under 10 seconds, are in the RVC directory, we can start the \gls{rvc} UI within \gls{dvt} using \verb|rvc|. From there, we can start the training by entering the dataset directory and clicking two buttons. The GUI, together with the online documentation, is quite easy to follow and thus won't be explained further. This also counts for inferencing or converting voices with \gls{rvc}.

\section{Lip-Remapping}
 After we created the auditory part, we needed to adjust the visual part as well. This was done, in all cases but the unreal engine case, with the help of \gls{w2l} \cite{mukhopadhyayWav2LipAccuratelyLipsyncing2023}. Similar to the development of \gls{dvt}, we implemented the \gls{w2l} project as a portable Docker container as well. Different to \gls{dvt} the \gls{w2l} container features a Gradio interface only (see Figure \ref{fig:w2l-gradio}). This eases the file-handling process as the host filesytem is not required. Everything is stored inside the container during runtime and can be downloaded via the UI in the web browser.
 \begin{figure}[h]

  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/wav2lip/w2l-gradio.png}
  \caption{\gls{w2l} Gradio Webinterface}
  \label{fig:w2l-gradio}
\end{figure}

\textbf{Concrete Workflow} \\
The usage is fairly simple. Upload a video and corresponding audio to the interface and execute \gls{w2l}. To create the best results in regards of timing, it is advised to precisely edit the audio and video beforehand so that they have exactly the same length. \\
One downside of the public implementation of \gls{w2l} is that its lip synching model has been trained at a very low resolution of 96x96 pixels, which is not sufficient for today's standards of high definition. At a resolution of 1080p, a resolution of at least 256x256 would be required. To mitigate this problem, we've added a facial reconstruction library in the form of \textit{GFPGAN}, inspired by the Wav2Lip-GFPGAN GitHub project \cite{sainyAjaysainyWav2LipGFPGAN2023}.
The complete workflow is depicted in Figure \ref{fig:w2l workflow} below.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/wav2lip/w2l workflow.png}
  \caption{Workflow for Wav2lip}
  \label{fig:w2l workflow}
\end{figure}

Since the GFPGAN upsampling significantly increases the processing time, it can be disabled for quicker testing. Once finished, the video can be downloaded as an image sequence to a local computer. The reason why we chose to work with the image sequence instead of an already merged video, is that we wanted to reduce the video conversion touchpoints as we lose quality with every lossy video encoding. Lastly, the image sequences and audios need to be put back together in \gls{prpro}. Sometimes minor retouches were performed in \gls{ae}.

\textbf{Possibilities for quality improvements} \\
The process of GFPGAN upsampling every frame individually introduces a small flickering effect in the facial area of the final rendering. This could be mitigated further with deflickering software, but this is one step we chose not to carry out. \\ 
Alternatively to GFPGAN, it would have been possible to use the previously discussed face-swapping technologies (Section \ref{sec:face-swapping}) to upsample the \gls{w2l} results. We've tested the workflow using \textit{Inswapper}, and the results were not satisfying: 1) Using Inswapper on top of the blurry \gls{w2l} output, the result was blurry as well. It seems that Inswapper applies the blurriness to the generated image as well. This makes total sense in the case of motion blur, but is counterproductive in our case. 2) Using Inswapper on top of GFPGAN upscaled results did not work either. Still, a little bit of flickering could be perceived, probably because Inswapper tries to mimic the lighting as well as possible. \\
Probably a good solution would have been to use \gls{dfl}. There are some examples on the internet that confirm \gls{w2l} in combination with \gls{dfl} as a working approach. We decided against the use of \gls*{dfl} because of the added effort of training a \gls{dfl} model (2 weeks) and the significant post-processing needed for every one of our videos. 


\section{Stable Diffusion Video}
\label{sec:sd-video}
To further increase the degree of artificiality in our videos, we decideddecided to alienate the images with the use stablestable diffusion. In sectionSection \ref{sec:stable-diffusion-bg} we have established the image generator asas a versatile tool for many applications. The astonishing development can be credited to the active community byin large amounts. For our projectproject, we've used the \gls{auto1} stable Diffusiondiffusion front end with several extensions for better video processing. \\
There are two challenges with \gls{t2i}: \textit{control} and \textit{consistency}. A prompt to resemble a news anchor inside a TV studio will never remotely resemble our real anchorman. As a firstfirst step, the random seed needs to be turned off. To increase the consistency at least a bit, every frame needs the same random noise as a starting value. But this cannot help us withwith the movement of an object within the scene. In our casecase, it is especially important that at least the lips somewhat match to thethe spoken audionaudio. LuckilyLuckily, the \gls{sd}community had come up with a solution: ControlNet.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{./graphics/diffusion/ControlNet.png}
  \caption{ControlNet Examples \cite{foongIntroductionControlNetStable2023}}
  \label{fig:ControlNet}
\end{figure}

In addition to text prompts, ControlNet enables the user to input additional features (see Figure \ref{fig:ControlNet}) such as scribbles, various edge detection outputs, depth maps, normal maps, segmentation maps, and many more. The most important controller for our purposes, however, was \textit{OpenPose}, which is how we created results like in Figure \ref{fig:scheider-real-sd}. OpenPose extracts the gestures and facial expressions from the anchorman and feeds them into the diffusion process. For gestures and even finger movement, OpenPose works quite well. Regarding the facial movement, the OpenPose captures do not provide enough markers to track faces in high fidelity. Although a lot of facial expression details are lost in the process, it is enough to somewhat animate the mouth and eye movements.
Now, after we can mitigate the \textit{control} problem, we are facing the next issue. For a moving image impression, we need multiple frames per second; for digital video, 25 \gls{fps}, to be precise. Although ControlNet helps a lot in regards to the controlled categories (gesture and facial expressions) the rest of the image will look different in each frame. Also, fixing the seed value does not solve the consistency issues. Figure \ref{fig:controlnet-issues} shows some of these effects.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{./graphics/diffusion/ControlNet-issues.png}
  \caption{Consistency Problems: Studio Ghibli LoRa, fixed seed, ControlNet OpenPose}
  \label{fig:controlnet-issues}
\end{figure}

While the posture of the hands is often okay, the card he is holding frequently changes. These effects appear everywhere in the frame, like the color of his buttons or his tie. Watching these twitches at 25 \gls{fps} is very unpleasant and greatly reduces the overall quality of the video. \\
One way of fixing consistency is by using other ControlNet layers like depth maps or \textit{Canny}. A downside of increased ControlNet activity is reduced control using the text prompts. Consistency is also model-dependent, so we tried out various models. After a lot of trial and error, we ended up with the results seen in Figure \ref{fig:scheider-real-sd}.

\textbf{Concrete Workflow} \\
Prior to the \gls{sd} input, the base videos were created using Wav2Lip and \gls{tts} or \gls{rvc} as described earlier. We cropped the video to feature only the actor. This reduced processing time, increased quality, and improved consistency because the area of attention was smaller and more focussed. \\
After having generated the main actor, we composited him into the final clip using \gls{ae}. Because the background was  twitching as well, due to consistency issues, we decided to key it away, as only the actor is important for the scene. We accomplished this by generating a depth map with \gls{auto1} and using the map as the isolation criterion. Afterwards, we composited the actor on a studio image background. The complete \gls{sd} workflow is depicted in Figure \ref{fig:sd-full-workflow}. 

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/diffusion/sd-workflow.png}
  \caption{Our Stable Diffusion Video Workflow}
  \label{fig:sd-full-workflow}
\end{figure}

At the time of writing, we can report that the ControlNet workflow might already be outdated. In July 2023, \textit{Animate Diff} was released and offers an alternative workflow to ControlNet. During the making of our experiments, Animate Diff was not tested and documented well enough, so we could not test it in a reasonable timeframe, but it seems in some cases to deliver much better results than ControlNet in both dimensions of control and consistency. In November 2023, Stability AI also released the first stable video diffusion foundation model, which shows a lot of future potential. In January 2024, Google release Lumiere, their video generation tool, which is just another example for the fast pace of current innovations.

\section{Game-Engine: Unreal Engine}
As another asset in our artificiality spectrum (4\_* in Figure \ref{fig:spectrum}) we decided to repurpose an existing project conducted in Unreal Engine. It was part of a master's practicum at LMU during the summer semester of 2023 under the supervision of Prof. Dr. Johanna Pirker. A detailed report of the practical is available as well, but we will include the most important findings in the following section. \\
The goal of the practical was to glimpse into the possibilities of virtual production, learning basic 3D modelling and the operation of \gls{ue} and accompanying software. To give the goal a concrete example, we wanted to recreate a news show from \gls{br} Television, which made it an ideal candidate for this paper as well. \\
Although it is quite easy to get started with \gls{ue}, realising complex projects is a difficult undertaking. The main reason lies in the fact that \gls{ue}'s main purpose is to aggregate many different assets and give them logic. Surely it is possible to model, rig, and animate characters within \gls{ue}, but there are various specialised tools for all of the named tasks: Maya, Zbrush, Blender, and iClone Character Creator, to name a few. Doing all of these things is certainly possible in \gls{ue}, but it has its limits. \\
Mastering many tools takes a lot of time, which is why we focused only on the integrated tools of UE. It is quite impressive what can be done with only \gls{ue} in approximately 200 hours of work, but it is also clearly visible that there is a lot of room for improvement. \\
The generated results have neither to do with \gls{genai} nor do they look particularly impressive, probably mostly because the character is quite uncanny. Still, we decided to employ the technology for our paper, and virtual production is expected to play a huge role in the coming year in regards to synthetic media. Because of its relevance to synthetic media, we decided to include thorough documentation of the unreal workflow. \\
We expected the uncanny avatar to produce the worst credibility due to the uncanny valley effect. This hypothesis proved to be wrong. For further details, please refer to chapter \ref{chap:study}. Briefly said, we assumed the uncanny appearance of the avatar being decisive for a bad reception. However, the Stable Diffusion generated examples rated even worse, mostly because the voice in the \gls{ue} examples was worse, due to it being the inferior \gls{tts} instead of \gls{v2v}.

\subsection*{Unreal Engine Basics}

Before we could start building anything specific, we had to learn some basics of \gls{ue}. There are plenty of free tutorials available online, so we first followed them. As can be seen from Figure \ref{fig:ue-basic-tutorial} the results can look quite impressive after a few hours of practice. This has to do with \gls{ue}'s excellent library of high-quality meshes (3D objects) and easy lighting system. However, things look very different as soon as one tries to create their own meshes, textures, or characters.

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/Basics/Landscape-Castle.png}
    \caption{Tutorial result: castle}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/Basics/Landscape-Overview.png}
    \caption{Tutorial result: landscape}
  \end{subfigure}

  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/Basics/Landscape-running.png}
    \caption{Tutorial result: character running}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/Basics/Texture.png}
    \caption{Tutorial result: basic texture}
  \end{subfigure}
  \caption{Results from first Tutorial}
  \label{fig:ue-basic-tutorial}
\end{figure}

\subsection*{Studio Build}

As we've just mentioned, modeling and texturing are extensive topics on their own. \gls{ue} excels at utilizing and rendering a wide range of objects, including a vast library of assets that can be used for free. Many of these assets are even scanned, providing incredible realism. However, when it comes to modeling custom objects, like the studio itself, achieving realism becomes much more challenging. Professional 3D artists would be required to achieve the desired level of accuracy and detail at this point. Nonetheless, it was fascinating to learn the possibilities that Unreal Engine offers, even when relying solely on the engine itself. \\
We built the studio with very few elements (see Figure \ref{fig:ue-studio-build}). A shiny white floor for reflective lighting, a semicircle backdrop, the main anchorman desk, and some generic studio lights at the ceiling. Due to our lack of knowledge about modelling and texturing, it does not look very realistic, but the similarities to the real counterpart are clearly visible. We tried to at least create a realistic replica of the main desk using photogrammetry (Figure \ref{fig:photogrammetry-desk}), but it did not work as expected due to the semi-transparent parts. As an alternative to photogrammetry, it would have been interesting to explore how \gls{nerf} or gaussian splatting would have performed in this domain, but due to time constraints, they couldn't be tested during the practical. \\
It is to be mentioned that \gls{genai} is becoming very useful in these domains, as \gls{sd} is already capable of creating 3D objects with ease. That way, modelling and texturing will very likely become much more accessible to semi-professional and amateur users.

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/studio/Studio-real.png}
    \caption{Real Studio for comparison}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/studio/Studio-Comparison.png}
    \caption{Studio recreation modelled in \gls{ue}}
  \end{subfigure}

  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/studio/studio-totale.png}
    \caption{Studio lighting}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/studio/Photogrammetry-Desk.png}
    \caption{Photogrammetry scan of desk}
    \label{fig:photogrammetry-desk}
  \end{subfigure}

  \caption{Unreal Engine TV Studio Build}
  \label{fig:ue-studio-build}
\end{figure}

\subsection*{Camera and Media Playback Logic} 

With the studio in place, a camera system needed to be implemented. \gls{ue} already provides support for cine cameras, which offer various settings reminiscent of their real-world counterparts, such as aperture, exposure, focal length, and focus. Initially, transitioning between cameras seemed straightforward using the integrated \textit{set view target with blend}. It was easy to set up multiple cameras within the scene and cut or animate between them using the mentioned method. This approach resembled the camera robots used in the real studio, utilizing easy-ease keyframes, which was advantageous. \\
However, there was a significant limitation with the existing method. While it effectively moved and rotated the view to the desired position, other parameters such as focal length, focus distance, and aperture did not change until the transition was complete. As a result, after the transition, these settings abruptly shifted, creating an undesirable visual effect. This behavior was unacceptable when aiming for smooth transitions. \\
To fix these issues, it was necessary to develop a separate camera system with custom control over the animation using \gls{ue}'s visual scripting language \textit{blueprint}. The scene now includes a master camera controlled by a blueprint script (Figure \ref{fig:blueprint}). This blueprint inherits methods for updating all relevant camera parameters, including a transition time that can be passed into the function call. 
\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{graphics/unreal-engine/blueprint.png}
  \caption{Camera Blueprint}
  \label{fig:blueprint}
\end{figure}
With the custom camera system in place, an endless number of cameras can be set up and animated between, which proves to be quite useful. Two examples of camera angles are depicted in Figure \ref{fig:cameras}.

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/camera angles/medium closeup.png}
    \caption{medium shot}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/camera angles/close up.png}
    \caption{medium close-up}
  \end{subfigure}
  \caption{Two virtual camera examples in the virtual studio}
  \label{fig:cameras}
\end{figure}


In many news formats, graphics presentations play a significant role. Analogously to the camera logic, we had to implement a blueprint for the media playback in the background of the anchorman (see Figure \ref{fig:ue-media}). These presentations can be implemented in various ways, such as changing the background image or displaying a floating image next to the host. To better understand how BR24 accomplishes this, we spent time with video engineers on the show. The complexity of their system was overwhelming: BR24 has hundreds of templates for how hosts can present content, all rendered in real-time using a \textit{viz.rt}graphics engine. \\
To keep the practical within a manageable scope, we decided to implement only one mockup display behind the news host, as depicted in Figure \ref{fig:ue-media}. The technical implementation of this feature is well documented in \gls{ue} tutorials. It involves using a \textit{media player} feature that renders a material onto a plane mesh. We've briefly considered using separate software like \gls{obs} to render graphics but opted against it. Introducing another piece of software would have added complexity and reduced customizability. It would have also run against the idea of doing as much as possible inside UE. \\
Similar to the camera system, additional functionality had to be added to the projection plane to use it in the desired manner. This was achieved by leveraging the knowledge gained from developing the camera system. Custom blueprints were created to control the behavior of the projection plane and expose callable methods. This enabled control over parameters such as opacity, fade-in/fade-out effects, loading and changing display images during runtime, and playing and pausing video media. \\
However, one issue remained with the projection plane: the images are not yet standardized, resulting in distortion. This will need to be addressed by applying appropriate adjustments to ensure the images appear undistorted. Additionally, camera positions need to be set to ensure the content in the background aligns appropriately with the projection.

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/media/slide-real.png}
    \caption{real media panel reference}
  \end{subfigure}
  \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{graphics/unreal-engine/media/slide-inplace.png}
    \caption{media panel visible}
  \end{subfigure}
  \caption{virtual media display}
  \label{fig:ue-media}
\end{figure}

\subsection*{Character Build}
\gls{ue} provides its own toolset for creating comprehensive virtual characters, so-called \textit{Metahumans}, which facilitates character creation to some extent. Like \gls{ue}, the Metahuman editor facilitates quick results but has limitations to modeling and texturing—customization options. To mitigate this problem, specialized software such as \textit{iClone Character Creator} or 3D animation software like \textit{Maya} or \textit{Blender} could be utilized. During the practical, these alternatives could not be covered, which is why we relied solely on \gls{ue}'s Metahuman editor. \\
Metahumans are currently undergoing active development and evolving rapidly. Editing Metahumans is done through a cloud-rendering web application in the browser, which is then synced to \gls{ue}. Creating a new character can be done from scratch or based on a real human; however, the method to achieve this changed over the course of several weeks, which shows the pace of development. In December 2023, \textit{gaussian avatar} will be available for the first experimental implementation. This will drastically improve future workflows. \\
For our case, we initially had to employ a workflow called "mesh to metahuman", which required a 3D mesh of a head to be imported into \gls{ue}. From there, it could be converted into a metahuman. We captured a head scan using the same photogrammetry workflow as with the studio table. The scan yielded exceptional results, as depicted in Figure \ref{fig:head-photogrammetry-scan}. \\
Unfortunately, the resulting metahuman character did not bear much resemblance to the scan, as shown in Figure \ref{fig:metahuman-result}. With more expertise in character modeling, it may be possible to improve the likeness to a large extent. \\
During our research, \gls{ue} version 5.2 was released, introducing a new workflow that utilizes the faceID lidar sensors of an iPhone to capture facial performance data. The results were much better than the "mesh to metahuman" workflow. Still, the Metahuman character creator lacks customization options in various aspects, hindering us from achieving higher likeness.

\subsection*{Character Animation}

Next, we needed to bring the character to life through animation. There were several options for achieving that objective, including using prerecorded performance data or implementing a live capture workflow. For the news format, a responsive and quick-to-produce approach was desirable, making the live version preferable. However, implementing a full live capture system would require extensive tracking hardware, such as a full-body tracking suit. These suits can range in cost from 2.000€ to 5.000€ and can be complex to calibrate. Given the time constraints, a simpler and more cost-effective solution was chosen for the prototype. \\
Firstly, facial tracking using the iPhone's FaceID sensors. This allows for tracking of facial expressions and head rotation. Unreal Engine provides easy support for this, requiring only an iPhone on the same network as the Unreal PC and sometimes necessitating custom firewall settings. One challenge, however, is directing the gaze of the eyes towards a virtual camera. If the eyes do not look into the camera, the character can appear uncanny. 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{graphics/unreal-engine/MH/bent-head.png}
  \caption{Uncanny Results from our animated Metahuman}
  \label{fig:uncanny mh}
\end{figure}

Secondly, the rest of the body received an idle animation in the form of simple breathing. That way, the body looked more natural while the face was driven by an actor recording the moderation using the iPhone FaceID sensor. The end result has a lot of room for improvement. Especially the face animations look quite uncanny (see Figure \ref{fig:uncanny mh}). The eyes don't focus the camera, and only a few facial landmarks are animated. This has to do with the technical limitations of the facial capture, as there are too few facial landmarks being tracked. \\
All in all, the performance falls deep into the uncanny valley, making it the perfect example for our negative control group.

\chapter{Study}
\label{chap:study}

During our research and work in the film and television industry, we have encountered various anecdotal assessments about trust in synthetic media. In regards to the scarce research landscape on such a young topic, we decided to conduct our own exploratory study in the area of synthetic media trust and credibility. \\
As there are many different ways of creating synthetic media, we wanted to test multiple methods against each other and see how they performed. This resulted in our main research question (RQ1) about how the artificiality of synthetic media influences trustworthiness. Other questions in neighbouring fields were postulated along the way (refer to \ref{tab:research-questions}).

\begin{table}[h]
  \centering
  \begin{tabularx}{\textwidth}{l|X}
    \textbf{Research Question/Hypothesis} & \textbf{Description}\\
    \midrule
    RQ1a & Will video artificiality have an impact of trustworthines?  \\
    \midrule
    RQ1b & Does age, education or profession impact the effects in RQ1a?  \\
    \midrule
    RQ1c & What effect does the screen size have on the effects in RQ1a?  \\
    \midrule
    RQ2a & What effect does an "AI Logo" label have?\\
    \midrule
    RQ2b & Does age, education or profession impact the effects in RQ2?  \\
    \midrule
    RQ2c & What effect does the screen size have on the effects in RQ2?  \\
  \end{tabularx}
  \caption{Research Questions and Hypothises}
  \label{tab:research-questions}
\end{table}

\section{Study Design}
\label{sec:study design}

Our goal was to find out more about the relationship between various types of synthetic media and their trustworthiness. In Chapter \ref{chap:background}, we have discussed the relevant technologies we have acquired over time. With access to these tools, we wanted to design a study that utilizes them in a practical scenario that is close to a daily practice use case. We therefore chose the TV newscast as a good testing environment. It provided sufficient control and the ability to experiment with \gls{tts}, \gls{v2v}, \gls{sd}, or even \gls{ue}. \\
The newscast sets the content. We chose to build upon existing news and not invent news in order to keep the content realistic. This should have eliminated bias towards unrealistic news, but unfortunately, we did not take into account differing knowledge. We will address this issue later while discussing the limitations (\ref{subsec:limitations}) of our study.

We decided to craft our study around the aforementioned spectrum of artificiality, as depicted in Figure \ref{fig:spectrum}. By introducing an increasing amount of synthetic media at each stage, we were hoping to measure the effects on trust. Artificiality, therefore, was one of our most important independent variables. Artificiality has close ties to the uncanny valley effect. Using several questions, we calculated a perceived index for each video. To avoid ambiguity with the term AI, we will name this index \gls{ri} instead of "artificiality Index". High \gls{ri} is analogous to low artificiality and a low uncanny valley effect. To ensure clear naming, we will refer to all these concepts only by the term \gls{ri} and avoid the other terms from now on.

We anticipated that the content itself would also have some effect on the judgements. To mitigate these issues, we created a second video at each quality level but filled it with different content. That way, we were able to control and compare results in both content groups. \\
Discussions about \gls{genai} led quickly to the call for marking footage that has been created with AI. We wanted to address this topic and decided to test for it as well. We did this by placing an AI logo in the upper corner of half of the videos. The meaning of the AI logo was explained in advance. To be ethically correct, we informed the participants that the AI logo marks AI-generated content in some cases but can also be wrongly marked in other cases. While our study was already running, we discovered the paper by \citeauthor{toffTheyCouldJust2023}, in which the authors conducted similar testing in the domain of AI-generated texts. Unfortunately, we couldn't incorporate their findings to optimize our study design. But since the AI marking experiment was just an add-on to our main research question, this wasn't too much of a missed opportunity. Please refer to Table \ref{tab:video-table} for a clear overview of the artificiality level, content, and AI logo marking strategy. 

\begin{table}[h]
  \centering
  \begin{tabularx}{\linewidth}{c|c|X|X}
    \textbf{ID} & \textbf{AI Logo} & \textbf{Content} & \textbf{Artificiality}\\
    \midrule
    1\_1 & in group B & German unification day celebrations  & None: real video from broadcast \\
    \midrule
    1\_2 & in group A & USA, McCarthy impeachment  & None: real video from broadcast \\
    \midrule
    2\_1 & in group B & Smartphone usage decreased at Oktoberfest  & audio generated with \gls{v2v} conversion, video real+\gls{w2l} \\
    \midrule
    2\_2 & in group A & Introduction of new traffic management system  & audio generated with \gls{v2v} conversion, video real+\gls{w2l} \\
    \midrule
    3\_1 & in group B & Youth music festival in bavaria  & audio generated with \gls{tts}, video real + \gls{w2l} + style transformation with \gls{sd} \\
    \midrule
    3\_2 & in group A & US democratic party under pressure  & audio generated with \gls{tts}, video real + \gls{w2l} + style transformation with \gls{sd} \\
    \midrule
    4\_1 & in group B & Robert Habeck visits Gamescom  & audio generated with \gls{v2v} conversion, video filmed in \gls{ue} \\
    \midrule
    4\_2 & in Group A & Biden visits middle east  & audio generated with \gls{v2v} conversion, video filmed in \gls{ue} \\
  \end{tabularx}
  \caption{Description of our test Videos}
  \label{tab:video-table}
\end{table}

Finally, we needed to layout our test videos in a specific way to avoid side effects measured by the content of the videos, the order of the videos (sequence bias) and the AI marking of the videos (see Table \ref{tab:video-table}). \\

In total, there were eight videos, two for each of the four artificiality stages. In Group A, the second video within an artificiality stage was marked by the AI logo. Group B had it reversed, meaning the first video of each stage was marked. \\
During interrogation, each participant was randomly assigned to group A or B. This measure was meant to compensate for side effects created by content variation. The size of the groups was continuously balanced so that Groups A and B stayed at similar sizes. \\
Within each group, the participant was presented with all eight videos, one after the other. The order of the videos was randomized for each participant in order to mitigate sequence bias. All questions in regards to a specific video were answered alongside the concerning video, while no other video was present. This, in addition to the random sequence, was meant to reduce direct comparability effects.

In addition to the video questionnaire, we've asked for basic demographic information. Our study encompasses both a between-subject design (A/B Group division) and a within-subject design (analysis within Group A or B). However, the sample size within the groups was probably too small for certain tests.

\section{Procedure and Participants}
\label{sec:procedure-and-participants}

The study was designed as an online questionnaire, conducted via the survey platform "soSci survey", hosted by the University of Munich. The survey was reachable via a single URL. The URL was spread among various media companies, university groups, and the author's personal contacts. \\
After following the URL, a visitor was immediately assigned to group A or group B of the questionnaire. The survey itself was divided into three stages: 1) general and democratic questions; 2) main video evaluation; 3) post-questionnaire evaluation and study explanations. \\
During the second stage, all videos were presented to the participant in a randomized sequence. Along with each video, the participant had to answer five questions on a Likert scale of \textit{one} to \textit{six}.
\begin{enumerate}
  \item The content of this video is true.
  \item This video seems trustworthy.
  \item The anchorman is real.
  \item The anchroman's voice is real.
  \item The video has a good quality.
\end{enumerate}
In addition to the Likert scale, the participants also had the opportunity to answer any of the questions with additional free text remarks. \\
During the post-questionnaire, we questioned how focused the participants were during the survey and how seriously they listened to the videos. On a scale of \textit{one} to \textit{three}, we received a meaningless mean of 1.138 (closer to 1 is better) and a distraction mean of 1.318 (closer to 1 is better). On a four-step scale of how much the participants enjoyed the study (from \textit{not at all} to \textit{very much}) we received a mean of 3.185 (closer to 4 is better). According to these values, we infer that our gathered data accurately represents our participants meaning and isn't skewed by their unwillingness to participate. \\
After all questions were answered, we provided an explainatory video for the study with a call to further spread the survey among friends and colleagues. We further incentivized participants to take part in the study by offering 50€ to be given away in a lottery after the study has been finished.
In total, we could gather N=195 valid participants. Regarding the age distribution, the median lies at 30-34 (see \ref{fig:age-distribution}).

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{graphics/statistics/age-plot.png}
  \caption{Sample Age Distribution}
  \label{fig:age-distribution}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{graphics/statistics/occupation.png}
  \caption{Sample Occupation Distribution}
  \label{fig:occupation-plot}
\end{figure}

The distribution of industrial sectors show a focus on IT (24.6\%) and creative media (39.5\%) (Table \ref{fig:piechart-sectors}) while most participants were currently employed (50.3\%) or in education (28.2\%) (Figure \ref{fig:occupation-plot}). 

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.58\textwidth}
    \includegraphics[width=\linewidth]{graphics/statistics/piechart-sectors.png}
    \caption{Sample's Affiliation to Industry sectors}
    \label{fig:piechart-sectors}
  \end{subfigure}
  \begin{subfigure}{0.38\textwidth}
    \includegraphics[width=\linewidth]{graphics/statistics/piechart-devices.png}
    \caption{Distribution of used Devices}
    \label{fig:piechart-devices}
  \end{subfigure}
  \caption{Industry Sectors and used Devices}
\end{figure}

We have also collected data about the last educational institution graduated, gender, and country, but these factors had no relevant effects and thus could be omitted. \\
In regard to display devices, smartphones, laptop monitors, and larger external monitors were quite evenly distributed (compare Figure \ref{fig:piechart-sectors}). We will compare this self-assessment with our measurements later on. \\
Lastly, we asked participants which of the factors (video content, video quality or AI logo) had the strongest influence on their judgments in regards to perceived trustworthiness (see Figure \ref{fig:most-influence}).

\section{Results}
\label{sec:results}

Before we start with further analysis, we wanted to address the topic of handling our Likert scales. We are aware of the ongoing discussion about whether Likert scales can produce and be interpreted as metric data or if they can only supply ordinal data. The central argument against this is that equidistance between the options cannot be ensured. Devaluing these scales to ordinal data would prohibit arithmetic operations with the values as well as many statistical tests. Many Likert scales use a scale of four items, and there are recommendations to increase the number of Likert scale points to make them closer to continuous scales and normality \cite{wuCanLikertScales2017a}. We complied by using six points instead of four. In our next research, we will most likely use ten or eleven points, as suggested by \Citeauthor{hodgePhraseCompletionScales2007}. We further assume our Likert measurements to be metric; however, we checked some parametric tests with their non-parametric counterparts to ensure they wouldn't deliver drastically different results in regards to significance. In all of the conducted statistical tests this was never the case.

\subsection{Measuring Artificiality}

As mentioned in the study design, we wanted to measure the perceived artificiality to confirm our design of the artificiality spectrum was right. We did this by asking whether the anchorman's voice was true and calculating the mean of the two values. As mentioned earlier, we will further refer to this as the Realism Index \gls{ri}. \\
The mean values (Table \ref{tab:RIs-mean}) show that our artificiality spectrum worked almost as intended. We assumed (Figure \ref{fig:spectrum}) that the unreal engine example would rank last because it looked the most uncanny. But instead, the perceived \gls{ri} shows that, in fact, the unreal engine videos ranked third, while the stable diffusion videos from group 3\_* ranked last. The reason for our misjudgment is clear. We thought the uncanny character of the unreal engine example would have the most influence, but actually the very synthetic-sounding voice of the 3\_* videos reduced the \gls{ri} strongly. To make matters worse, 3\_* videos were altered using \gls{sd}, which made them even more artificial. In summary, these videos felt less good compared to the 4\_* videos.

\begin{table}[h]
	\centering
	\caption{RIs mean Value Ranking}
	\label{tab:RIs-mean}
	{
		\begin{tabular}{lrrrrrrrrrr}
			\toprule
			 & 1\_1 & 1\_2 & 2\_1 & 2\_2 & 4\_1 & 4\_2 & 3\_1 & 3\_2 & *\_1 & *\_2 \\
			\cmidrule[0.4pt]{1-11}
			Mean & $5.39$ & $5.38$ & $2.7$ & $3.28$ & $1.92$ & $2.09$ & $1.64$ & $1.54$  & $2.93$ & $3.07$\\
			\bottomrule
		\end{tabular}
	}
\end{table}
\begin{figure}[h]
  \includegraphics[width=1\textwidth]{graphics/statistics/RIs/RI_compilation.png}
  \caption{Distribution of RIs: individual Videos and grouped}
  \label{fig:all-RIs}
\end{figure}

Figure \ref{fig:all-RIs} plots the \gls{ri}s for each video across both subgroups (A and B). \\
It lies in the nature of the questions that the resulting RI data is heavily skewed in many cases (figures in \ref{fig:all-RIs}). Naturally, we assume that most people will regard something as real if it is real. This would usually prohibit the assumption of normality. To mitigate that, we've combined all RIs for videos *\_1 and *\_2 and cross-checked some tests with this combined distribution. As seen in figure \ref{fig:all-RIs}, they can be considered to be normally distributed. Shapiro-Wilk tests deliver p-values of 0.31 for RI *\_1 and 0.024 for RI *\_2, while all other non-grouped \gls{ri}s have p-values of < 0.001. In addition, the central limit theorem for larger sample sizes (n > 30) considers normality to be less critical. With our sample size of 195 we will continue using parametric tests on all RI values.

\subsection{RQ1: Artificiality and Trust Relationship}
\label{subsec:RQ1}

Tables \ref{tab:trust-RI1-correlations} and \ref{tab:trust-RI2-correlations} show a significant positive correlation in each of the expected cases, while the correlation strengths cover everything from weak (0.251) to strong (0.617). Therefore, we can confirm Hypothesis H1a. \\
One lesson for media companies from this could be that the highest possible quality should be strived for before such content goes online to ensure the highest trust ratings. However, we must keep in mind that the tests only cover subject testing. As every participant has seen all of the contents, even in random order, they are clearly influenced by having the other videos for reference. \\
It might be interesting to conduct further studies with isolated groups, where only one level of quality is tested. It might be possible that artificially looking content could score similar trust results as realistic content when there is no comparison in the participant's frame of reference.

\begin{table}[h]
	\centering
	\caption{Pearson's r Correlations trust - RIs 1}
	\label{tab:trust-RI1-correlations}
	{
		\begin{tabular}{lrrrrr}
			\toprule
			Variable &  & 1\_1\_trust & 2\_1\_trust & 4\_1\_trust & 3\_1\_trust  \\
			\cmidrule[0.4pt]{1-6}
			1\_1\_RI & Pearson's r & \textbf{0.617} & $0.006$ & $0.022$ & $0.080$  \\
			$$ & p-value & \textbf{$<$ .001} & $0.931$ & $0.756$ & $0.264$ \\
			2\_1\_RI & Pearson's r & $0.107$ & \textbf{0.444} & $0.153$ & $0.224$ \\
			$$ & p-value & $0.135$ & \textbf{$<$ .001} & $0.032$ & $0.002$\\
			4\_1\_RI & Pearson's r & $\approx 0$ & $0.081$ & \textbf{0.312} & $0.263$   \\
			$$ & p-value & $0.990$ & $0.259$ & \textbf{$<$ .001} & $<$ .001 \\
			3\_1\_RI & Pearson's r & $0.168$ & $0.053$ & $0.132$ & \textbf{0.256} \\
			$$ & p-value & $0.019$ & $0.466$ & $0.066$ & \textbf{$<$ .001} \\
			\bottomrule
			% \addlinespace[1ex]
			% \multicolumn{10}{p{0.5\linewidth}}{* p $<$ .05, ** p < .01, *** p < .001} \\
			% \multicolumn{10}{p{0.5\linewidth}}{* $$} \\
			% \multicolumn{10}{p{0.5\linewidth}}{*** $$} \\
			% \multicolumn{10}{p{0.5\linewidth}}{** $$} \\
		\end{tabular}
	}
\end{table}
\begin{table}[h]
	\centering
	\caption{Pearson's r Correlations trust - RIs 2}
	\label{tab:trust-RI2-correlations}
	{
		\begin{tabular}{lrrrrr}
			\toprule
			Variable &  & 1\_2\_trust & 2\_2\_trust & 4\_2\_trust & 3\_2\_trust  \\
			\cmidrule[0.4pt]{1-6}
			1\_2\_RI & Pearson's r & \textbf{0.598} & $0.109$ & $0.103$ & $0.009$  \\
			$$ & p-value & \textbf{$<$ .001} & $0.065$ & $0.076$ & $0.448$ \\
			2\_2\_RI & Pearson's r & $0.051$ & \textbf{0.606} & $0.197$ & $0.174$ \\
			$$ & p-value & $0.238$ & \textbf{$<$ .001} & $0.003$ & $0.007$ \\
			4\_2\_RI & Pearson's r & $0.014$ & $0.009$ & \textbf{0.300} & $0.003$   \\
			$$ & p-value & $0.990$ & $0.259$ & \textbf{$<$ .001} & $<$ .001 \\
			3\_2\_RI & Pearson's r & $0.024$ & $0.043$ & $0.128$ & \textbf{0.251} \\
			$$ & p-value & $0.369$ & $0.273$ & $0.037$ & \textbf{$<$ .001} \\
			\bottomrule
			% \addlinespace[1ex]
			% \multicolumn{10}{p{0.5\linewidth}}{* p $<$ .05, ** p < .01, *** p < .001} \\
			% \multicolumn{10}{p{0.5\linewidth}}{* $$} \\
			% \multicolumn{10}{p{0.5\linewidth}}{*** $$} \\
			% \multicolumn{10}{p{0.5\linewidth}}{** $$} \\
		\end{tabular}
	}
\end{table}

As for the research question RQ1b, we could not find any significant differences comparing groupings by age, education, or profession. Conducting MANOVA tests by age as the fixed factor delivered hints for significant differences (p-value: 0.005) but a more detailed look using ANOVA never showed any significance. This is most likely due to the small sample sizes after division into subgroups. \\
Creating subgroups by education or profession causes even smaller subgroups, which is why many calculations could not be performed properly. Also, there are many disturbances that clearly overshadow any effect caused by age, education, or profession differences. To summarize, we can state that there are differences between the subgroups overall, but we cannot reliably say where and in which direction. Additionally, the significance quickly disappears in our data. It is very clear that an isolated study would be needed to target these sorts of questions that our exploratory study found hints of.

Reading through some of the comments on our study and talking informally with subjects, we gained the feeling that display size could have some impact on the results. Logically, a smaller screen would make visual imperfections less visible, resulting in better \gls{ri}s for a given video. While testing for RQ2c (does screen size have an effect on the \gls{ri}s) we can state similar findings as for RQ1b: Using the MANOVA, we can state that there are differences (p-value: 0.039) in RIs considering the used devices.
In ANOVA tests, only one of the cases was significant (2\_2\_RI: p-value 0.024). Post-hoc analysis showed one only significant case: smartphone vs. laptop, with a mean difference of +0.719 and a p-value of 0.02. This would support our assumption that a smaller display causes a (very) slightly higher \gls{ri}. \\
We don't think that this one result represents any convincing evidence, as all other cases are insignificant, but together with the verbal discussion, we still believe there are hints that this topic could be researched further, especially with mobile devices becoming more and more important.

\subsection{RQ2: Effects of AI Disclosure to Trust}
\label{subsec:RQ2}
We wanted to get an understanding of what effect the disclosure of a possibly AI-generated video might have. A first self-assessment by the participants implies that there is a very minor effect of the markings (compare Figure \ref{fig:most-influence}).
\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{graphics/statistics/most-influence.png}
  \caption{Sample's Judgement about most influential Trust Factors}
  \label{fig:most-influence}
\end{figure}
As mentioned in Section \ref{sec:rel-studypart}, \Citeauthor{toffTheyCouldJust2023} discovered statistically significant reduction of perceived trustworthiness, perceived accuracy, and perceived fairness. 

Their study observed AI-generated texts, which are much more controllable than audio-visual content. Still, we were interested in comparing our results with theirs. \\
Before examining our data, we crafted two working theories about what we would expect to measure. The theories were based on previous testing, anecdotes, and assessments by various parties. We expected to see a drop in perceived trust in good-quality videos (1\_* and 2\_* videos) but an increase in bad-looking examples (4\_* and 3\_* videos). Our expectations followed a certain logic:

1) Convincing-looking videos, once marked as AI-generated, would decrease due to present prejudices about AI. Fears about an unrecognized Deepfake would fuel such thoughts. \\
2) On the other hand, bad-looking videos would have a low trust value as a starting point, which we confirmed with RQ1. Once such a video is revealed as AI-marked, a viewer will accept its inferior quality because the reason for the inferiority is now clear. An effect like in 1) would not appear because the content had bad quality in the first place. A rather contrary, relieving feeling would be expected, as the presented AI is "just not good" and therefore less scary than in the first scenario.

Clearly, these two scenarios are speculations or, at best, an educated guess. It was questionable, given the design of the study, if any significance could be measured in the presence of many other disruptive factors.

As for the statistical evaluation, we conducted Student's "T-Tests" and Mann-Whitney "U-Tests" in case the normality of our data should have been questioned. The dependent variables were the \gls{ri} values, grouped by the condition, whether a particular video was marked or not. Grouped RIs (*\_1 and *\_2) are the same accumulated, normally distributed values as introduced Figure \ref{fig:all-RIs}.
\begin{table}[h]
	\centering
	\caption{AI markings: T-Test/U-Test}
	\label{tab:ttest_logo-trust}
	{
		\begin{tabular}{lrr|lrr}
			\toprule
			\textbf{Case} & \textbf{p (t-test)} & \textbf{p (u-test)} & \textbf{Case} & \textbf{p (t-test)} & \textbf{p (u-test)}  \\
			\cmidrule[0.4pt]{1-6}
			1\_1\_trust & $0.091$ & $0.137$ & 1\_2\_trust & $0.487$ & $0.397$ \\
			2\_1\_trust & $0.390$ & $0.417$ & 2\_2\_trust & $0.481$ & $0.465$ \\
			3\_1\_trust & $0.771$ & $0.588$ & 3\_2\_trust & $0.983$ & $0.737$ \\
			4\_1\_trust & $0.698$ & $0.512$ & 4\_2\_trust & $0.152$ & $0.077$ \\
			grouped RI *\_1 & $0.800$ & $0.788$ & grouped RI *\_2 & $0.753$ & $0.735$\\
			\bottomrule
			% \addlinespace[1ex]
			% \multicolumn{4}{p{0.5\linewidth}}{\textit{Note.} Student's t-test.} \\
			% \multicolumn{4}{p{0.5\linewidth}}{$^{0}$ Brown-Forsythe test is significant (p $<$ .05), suggesting a violation of the equal variance assumption} \\
		\end{tabular}
	}
\end{table}
Unfortunately, we couldn't measure any significance in any of the categories (compare with Table \ref{tab:ttest_logo-trust}). With our study design, which was focused on measuring the overall effects of increasing artificiality, this was to be expected. 

The varying video quality has too many strong effects, overshadowing any effects an AI marking could have. We therefore cannot confirm nor deny the results from \Citeauthor{toffTheyCouldJust2023}.\\
By examining comments, we also figured that we had created quite some differences between videos of one quality stage by having chosen differing topics or fields on the content side. The choices of content should have been made more neutral, or at least more similar, within one quality stage. \\
In a follow-up study where this has been improved, it might be easier to measure the effects of the AI logo. Just like \Citeauthor{toffTheyCouldJust2023}, the groups between marked and unmarked must be comparable in both quality and content.

We can conclude that the participants' self-assessment, depicted in Figure \ref{fig:most-influence}, seems to be accurate. We cannot measure any significant effects regarding the AI mark as the other variables introduce too many disturbances. \\
In regards to the direction of the effects, it is also impossible to state anything robust. Judging by our experiences, we are still not ready to omit our aforementioned working theories, but for further research in this direction, a different study design would be required.

 \subsection{Exploratory Factor Analysis}
\label{subsec:factor-analysis}

To further inspect the quality of our questionnaire, we've conducted an exploratory factor analysis (Table \ref{tab:factorLoadings}) for our main questions. The results show that all measurements somewhat point to the same latent variable. This surely conforms to the correlations we have found in the previous sections. Even if the questions all point towards the same measurement, they were interesting to us because all of the variables are things we could potentially manipulate in a follow-up study.

\begin{table}[h]
	\centering
	\caption{Factor Loadings}
	\label{tab:factorLoadings}
	{
		\begin{tabular}{lrr}
			\toprule
			Variable & Factor 1 (latent variable)  \\
			\cmidrule[0.4pt]{1-2}
			all seems-trustworthy & $0.900$  \\
			all video-quality is-good & $0.735$  \\
			all moderator is-real & $0.606$  \\
			all content is-true & $0.589$  \\
			all voice is-real & $0.560$  \\
			\bottomrule
			% \addlinespace[1ex]
			% \multicolumn{3}{p{0.5\linewidth}}{\textit{Note.} Applied rotation method is promax.} \\
		\end{tabular}
	}
\end{table}

\subsection{Discussion}

This study's exploration into the impact of synthetic media's artificiality on trustworthiness has yielded significant insights while also opening avenues for future research.

\subsubsection{Impact of Artificiality on Trustworthiness}
The findings indicate a clear correlation between the artificiality of synthetic media and its perceived trustworthiness. This supports the hypothesis that higher artificiality negatively impacts trust. Through statistical analysis, particularly using Pearson's r correlations, a clear and consistent correlation was identified: as the degree of artificiality in synthetic media increases, its perceived trustworthiness decreases. This trend was most evident in the calculated Realism Index (RI) values, where videos with higher \gls{ri}s, indicating lower artificiality, were consistently deemed more trustworthy by participants. \\
This pattern held across various forms of synthetic media, encompassing different alterations such as \gls{v2v} conversion, \gls{tts}, and Unreal Engine. Each form of synthetic media exhibited a distinct impact on trust, underscoring the complexity of how artificiality is perceived in different media contexts. The study's findings also highlighted the significant role of both visual and auditory elements in shaping perceptions of artificiality. The synthetic voices, in particular, in \gls{tts} and \gls{v2v} conversions, emerged as a crucial factor affecting trustworthiness ratings, whereas the inferior \gls{tts} always led to worse results than \gls{v2v} samples. \\
However, it's important to note that while the correlation between artificiality and trustworthiness was clear, the study's design may have limitations in capturing finer variations in trustworthiness at different artificiality levels. This observation calls for more detailed investigations in future research endeavors to understand the subtleties of this relationship better.

\subsubsection{Demographic Factors and Screen Size}
Notably, the research did not find significant effects based on demographics such as age, education, or profession. The study's sample, while diverse, might not have been sufficiently large or varied to detect subtle demographic differences in perception. This aspect of the research, while not conclusive, opens up interesting directions for future investigations. \\
Another intriguing aspect of the study was the exploration of the influence of screen size on the perception of synthetic media. While the results regarding screen size were statistically inconclusive, they hint at an underlying dynamic worth investigating. The hypothesis was that smaller screens might mask imperfections in synthetic media, potentially leading to higher trustworthiness ratings. This aspect of the research, though not definitively proven, suggests that the medium through which synthetic media is consumed – whether it be a smartphone, a laptop, or a larger display – could subtly influence how viewers perceive its authenticity and trustworthiness. \\
The implication of these findings, particularly regarding screen size, is noteworthy. In an era where media consumption varies widely across devices with different screen sizes, understanding how this variable affects perception could have significant implications for the design and dissemination of synthetic media. Future studies, therefore, should consider incorporating a more focused examination of screen size as a variable, potentially revealing more about how the context of media consumption impacts viewer perception and trust.

\subsubsection{Influence of disclosing the AI generated Nature of Content}
The absence of significant impact from AI logo branding on trust contradicts some existing narratives around AI transparency, like the works of \Citeauthor{toffTheyCouldJust2023}. This may indicate that viewer perceptions are more influenced by the content and presentation of the media than by disclaimers or labels (refer to Section \ref{subsec:RQ2}). \\
This finding suggests that the factors influencing viewer perceptions of synthetic media might be more complex than we assumed. It appears that the content itself and its presentation hold more sway over viewers' trust than the mere presence of disclaimers or labels indicating AI involvement. This could imply that viewers are either not significantly influenced by such labels or that the quality and nature of the content overshadow any concerns raised by the knowledge of AI involvement. \\
The absence of a significant difference in trustworthiness between AI-labeled and unlabeled content could also point to a potential desensitization among viewers to AI-generated content, as it becomes more prevalent in the media landscape. Alternatively, it could indicate a baseline level of trust or skepticism that is not easily swayed by labeling. \\
These insights open up new ideas for research, particularly in understanding how transparency about AI's role in content creation affects public perception. Future studies might delve deeper into the psychological and contextual factors that influence how viewers interpret and react to the disclosure of AI involvement in media production. The findings also raise important questions for practitioners in the field of synthetic media about the strategies they employ to build or maintain trust among their audiences.


\subsubsection{Exploratory Factor Analysis}
The factor analysis underscores the internal consistency of the questionnaire and the reliability of the constructs used, affirming the study's methodological approach (Section \ref{subsec:factor-analysis}). This analytical approach was instrumental in affirming the robustness of the study's methodological framework. By examining the factor loadings, it was possible to ascertain how well the individual questionnaire items correlated with the underlying latent variables (trust and artificiality) they were intended to measure. \\
The factor analysis revealed that all the main questions in the survey, designed to gauge various aspects of trustworthiness, consistently pointed towards a single latent variable. \\
As we've made clear, it must be noted that the questionnaire did not measure anything else well, besides trust. This is something that should be addressed in a follow-up study.

\subsection{Limitations}
\label{subsec:limitations}
Despite the study's contributions, several limitations must be acknowledged.

\subsubsection{Non-Representative Sample}
The study's sample recruitment approach, while suitable for its exploratory objectives, introduced a significant limitation concerning representativeness, as detailed in Section \ref{sec:procedure-and-participants}. This limitation primarily lies in the demographic and professional composition of the sample, which may not adequately represent the wider population's views on synthetic media. Also, the divided subgroups were simply too small, making any statistical measures impossible. \\
The demographic skew, predominantly towards individuals from the IT and creative media sectors, as shown in Figure \ref{fig:piechart-sectors}, raises concerns about the generalizability of the findings. Individuals in these sectors might have unique perspectives or familiarity with synthetic media, influencing their perception of trustworthiness differently compared to a more diverse audience.

\subsubsection{Limitation by Design}
The study's design, primarily hinged on within-subject comparisons, introduces a crucial limitation in discerning the isolated effects of individual factors on the perception of trustworthiness in synthetic media. This design choice, while beneficial for exploring a broad range of factors simultaneously, restricts the ability to ascertain the distinct impact of each variable. As a result, the intermingled effects of various elements like artificiality levels, AI logo branding, and content type might have influenced the participants' responses, as detailed in sections \ref{sec:study design} and \ref{sec:results}. \\
To address this limitation, future research in this area could benefit from incorporating a between-subjects design. Such a design would involve different groups of participants being exposed to different levels or types of synthetic media, allowing for a clearer assessment of how each variable individually influences perceptions. This approach would enable researchers to isolate the effects of each factor, providing a more nuanced understanding of the dynamics at play.

The influence of content and quality on the study's results cannot be overstated. The study found that the trustworthiness perceptions of synthetic media were heavily dependent on both the nature of the content and its production quality. This dependency poses a challenge in isolating the effects of other variables, such as the presence of an AI logo, on viewers' trust and perception. \\
By focusing on specific aspects of synthetic media in isolation, future research could uncover more detailed insights into how each component contributes to the overall perception of trustworthiness.

\chapter{Conclusion}
\label{chap:conclusion}

This study embarked on a nuanced exploration of the influence of artificiality in synthetic media on perceived trustworthiness, addressing pressing concerns in the rapidly evolving domain of AI-generated content. Through analysis and empirical testing, the study has uncovered critical insights, revealing a layered understanding of viewer perception in the context of synthetic media. During this process, elaborate workflows for the creation of various synthetic media applications have been formulated.

The primary findings of the study indicate that the quality of synthetic media exerts the most significant impact on trustworthiness, followed by content, and then the presence or absence of an AI logo. This hierarchy of effects underlines the complex interplay between different elements in shaping public perception. The study's in-depth analysis has also highlighted the limitations inherent in the current study's design, particularly in terms of sample representativeness and the ability to isolate specific factors for scrutiny.

Reflecting on these findings, it becomes evident that a singular study, such as the one undertaken, can only illuminate certain aspects of the broader phenomenon. To comprehensively understand each factor's individual impact, different studies focusing specifically on quality, content, AI logo presence, and other relevant elements are necessary. Moreover, future investigations might benefit from a more focused approach to content types and the varying degrees of participants' AI education. Such targeted studies could unravel finer nuances in viewer perception and trust, contributing to a more refined understanding of synthetic media.

In closing, this work is just part of the beginning of an extensive journey into understanding trust and credibility in the realm of synthetic and real media. The rapid advancements in AI and synthetic media technologies imply that these issues will continue to evolve, necessitating ongoing research and vigilance. As this disruptive process unfolds, it remains crucial to closely monitor how credibility and trust in media, both synthetic and real, develop in the coming years.

% End of main writing

\printbibliography
All links were last followed on \today{}.

\appendix
\chapter{Inswapper Examples}
\label{chap:insightface-demos}
The following tests were created using the \textit{Roop} Inswapper implementation. One image of the author's face was used for all face swaps. The included example images include Stills from these movies and music videos: \textit{Barbie} (2023), \textit{Oppenheimer} (2023), \textit{Iron Man} (2008), \textit{Juju feat. Henning May: Vermissen} (2019), \textit{Gotye: Somebody that I used to know} (2011).
These stills were chosen in order to test multiple lighting situations, angles etc. The results are very impressive in most situations. However Close-up shots often don't work due to Inswapper's resolution of only 128x128 pixels. Also the masking of objects in front of the face is often inferior to those of \gls{dfl}.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/inswapper/multiple1.png}
  \includegraphics[width=1\textwidth]{./graphics/inswapper/multiple2.png}
  \caption{Tested multiple Faces at once, male and female.}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/inswapper/oppenheimer2.png}
  \includegraphics[width=1\textwidth]{./graphics/inswapper/kimbra.png}
  \caption{Color Transfer to the Target works great.}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/inswapper/iron-man-too-close.png}
  \caption{CloseUp: Resolution does not suffice anymore for FullHD Material}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{./graphics/inswapper/oppenheimer1.png}
  \caption{Side View of Faces often does not work.}
\end{figure}

\chapter{Brief Stable Diffusion explanation}
\label{app:diff-workflow}
A good explanation of how the Stable Diffusion process works can be found at \url{https://stable-diffusion-art.com/how-stable-diffusion-work/}. \\
Explained very briefly, as depicted the in Figure \ref{fig:forward-diff}, a neural network is trained to consecutively add noise to an image until it is just random noise. During the process, word embedding information about the image is kept and also fed into the training process.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/forward-diff.png}
  \caption{Forward Diffusion Process during training \cite{andrewHowDoesStable2022}}
  \label{fig:forward-diff}
\end{figure}

During inference, the process is then reversed. Starting from random noise with added word captions, the neural network is tasked with removing the noise until the image is clear. That said, the output image is mainly dependent on the input prompt and the random starting noise (seed).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{./graphics/reverse-diff.png}
  \caption{Reverse diffusion process \cite{andrewHowDoesStable2022}}
  \label{fig:backward-diff}
\end{figure}

For further information, please visit \url{https://stable-diffusion-art.com/how-stable-diffusion-work/}.

% \input{latexhints/latexhints-english}

\pagestyle{empty}
\renewcommand*{\chapterpagestyle}{empty}
\Affirmation
\end{document}
